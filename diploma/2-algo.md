# Часть 2 (Алгоритмы)
## New
### Словарь
* Узел - 1 сервер, на который приходят запросы пользователя/который участвует в алгоритме
* Реплика - сервер с копией данных нашего хранилища
* Участники - все ноды системы, клиент.
* Чекер - TLC
* Выполнение - бесконечная последовательность состояний системы. (Трасса/траектория)

### Реактивность
Распределенная система описывается набором своих состояний. Это соотвествует вершине в грфе конфигураций

Система меняется, когда какой-то узел реагирует на доставленное сообщение от других узлов. Это сооьветсвует переходам в графе конфигураций.

Реакция узлов выражена в виде экшенов, которые отвечают за получение сообщения, изменения состояния системы и отправки нового сообщения.

Например: BecomeLeader, FollowerReplicate, Restart, DuplicateMessage, ClientRequest

### Уровень абстракции
Распределенная система представляет собой отдельные микросервисы / узлы, которые играют разные роли, и живут в модели message passing. Для пользователя распределенная система - это некоторый сетевой адрес, по которому отправляются команды (Get/Set для k/v хранилища, Create/Append/Delete для файловых систем и т.п.) и ответ, чаще всего через некоторую клиентскую библиотеку или простой crud, так что для пользователя такая система - конкурентный / многопоточный объект, а сетевой адрес - это имя этого объекта. Пользователь работает в модели shared memory.

Все это справедливо не только для самой системы, но и для отдельных ее компонентов. Например: Kafka использует ZooKeeper для хранения состояния кворума. Его можно рассматривать как отдельную ячейку в модели shared memory, так и как набор узлов, которые между собой взаимодействуют.

В самом начале надо понять: какие компоненты системы надо моделировать, а какие нет. Например в ClickHouse мы не моделируем log в ZooKeeper, а считаем, что это просто атомарная ячейка памяти. А в Raft log явно представлен на всех репликах.

Дальше следует выделить основных участников системы. Ими могут быть ноды (Паксос, Raft) или сущности, над которыми происходит воздействие(транзакции в SI).

Сравним существующие системы и их спеки:

Paxos/Raft: В этих алгоритмах основными участниками являются ноды. Мы хотим описать алгоритм общения между ними, моделируя передачу сообщений, сеть и т.д. Это самый высокий уровень детализации. Мы хотим полностью описать, как ведут себя участники алгоритма, их протокол взаимодействия и проблемы, с которыми они сталкиваются. В системах, где одной из составляющих является обмен сообщениями по сети надо уметь моделировать асинхронность сети.

Kafka: Контроллер используют Zookeeper для упорядочивания апдейтов кворума. Для узлов, которые выполняют роль контроллера Zookeeper - это отказоустойчивое глобальное разделяемое состояние, которое наблюдают все участники, для него znode - это распределенная отказоустойчивая ячейка памяти, которую изменяют узлы. Можно провести аналогию между ZNode в ZooKeeper-е и атомиком в C++. Создатели сами проводят такое сравнение: "Our system, Zookeeper, hence implements an API that manipulates simple wait-free data objects organized hierarchically as in file systems", так как wait-free  - это свойство конкурентных объектов. Вот, что сам автор спеки пишет по этому поводу: "This is the model's equivalent of the state in Zookeeper, but generally we ignore the complexity of Zookeeper itself. Instead we allow simple atomic operation to the state directly within individual actions". Поэтому, это просто variable и в спеке участники будут ее изменять в экшенах. Единственное место, где мы хотим описать взаимодействие реплик - это обмен сообщениями во время вставки, так как она подтвердится клиенту, только когда все реплики из ISR ее применят к себе.

Percolator - это протокол ckient side расаределенрныз транзакций, который использует BigTable. Big Table - это распределденное отказоустойчивое хранилище с которым клиенты взаимодейтсвую. И для них ячейки памяти в хранилище - это атомики. Все взаимодействие между клиентами спрятано внутри Big Table. В этой спеке самый высокий уровень абстракции, в котором нет никакого взаимодействия по сети между узлами системы.

Первый шаг работы над спецификаций системы -  выбор уровень детализации разных компонент.

### Моделирование мира
Займемся моделированием мира, в котором функционирует наша система: опишем, как функционирует сеть, с какой скоростью работают узлы, как течет время и т.п.

Есть 2 модели: асинхронная и синхронная. В первой нет ограничений на
* время доставки сообщений
* дрейф часов
* относительную скорость работы узлов.

В синхронной модели эти ограничения есть.

Реальность где-то посередине. Например: реальная сеть большую часть времени ведет себя синхронно, но никакая сетевая инфраструктура не может гарантировать это на 100% времени. Бывают периоды нестабильности, когда сеть задерживает и теряет сообщения.

Мы будем использовать асинхронную модель. Мы сильно пессимизируем, когда описываем систему в этой модели, так как в реальности сеть не бывает бесконечно долгой. Это можно делать, так как выполнения в реальном мире - это подмножество в выполнениях в асинхронной модели, поэтому, если мы убедимся в корректности алгоритма/системы в модели без ограничений, то и с ними он будет работать. Так же описать асинхронную модель проще.

Распределенные системы скрывают отказы внутри себя и остается доступны после падения нескольких узлов и мы хотим в этом убедиться.

Поговорим подробнее про каждую из состовляющих моделию.

### Моделирование сети и отправки сообщений
Под моделирование сети мы понимаем механизм отправки и доставки сообщений.
В реальном мире ноды могут из-за внешних факторов не отвечать, или долго обрабатывать сообщения.

Сеть - это все провода, роутеры и прочее оборудоване. Когда мы говорим, что сообщение в сети - это значит, что оно находится где-то внутри оборудования, соедененных проводами.

В TLA+ все это описывает тип множества.

Пример

    VARIABLE msgs

Отправка собщения - это добавление его в это множество.

Пример:

В Paxos:

    Send(m) == msgs' = msgs \cup {m}

Отправка сообщения proposer-ом:

    Phase1a(b) == /\ Send([type |-> "1a", bal |-> b])
                  /\ UNCHANGED <<maxBal, maxVBal, maxVal>>

Получение собщение acceptor-ом и ответ на него:

    Phase1b(a) == /\ \E m \in msgs :
                      /\ m.type = "1a"
                      /\ m.bal > maxBal[a]
                      /\ maxBal' = [maxBal EXCEPT ![a] = m.bal]
                      /\ Send([type |-> "1b", acc |-> a, bal |-> m.bal,
                                mbal |-> maxVBal[a], mval |-> maxVal[a]])
                  /\ UNCHANGED <<maxVBal, maxVal>>

В Kafka:

    ControllerUpdateIsr(newLeader, newIsr) == \E newLeaderEpoch \in LeaderEpochSeq!IdSet :
        /\ LeaderEpochSeq!NextId(newLeaderEpoch)
        /\  LET newControllerState == [
                leader |-> newLeader,
                leaderEpoch |-> newLeaderEpoch,
                isr |-> newIsr]
            IN  /\ quorumState' = newControllerState
                /\ leaderAndIsrRequests' = leaderAndIsrRequests \union {newControllerState}


Конструкция \E val \in set: P(val). Выдает произвольный элемент множества, который удовлетворяет предикату. Тем самым порядок на получение элемента не гарантирован. Это помогает моделировать reordering на отправленных сообщениях и их задержку.

Например:

В Paxos проверяется, что у сообщениия, которое получено acceptor-ом на фазе 1b и 2b ballot больше, чем локальный у acceptor-а

    Phase1b(a) == /\ \E m \in msgs:
                      /\ m.type = "1a"
                      /\ m.bal > maxBal[a]
                      ...

   Phase2b(a) == \E m \in msgs : /\ m.type = "2a"
                                 /\ m.bal \geq maxBal[a]
                                 ...

В Raft-е при получении сообщения проверяется в каком term-е оно было отправлено, чтобы не обрабатывать сообщения из прошлого.

    HandleRequestVoteResponse(i, j, m) ==
        \* This tallies votes even when the current state is not Candidate, but
        \* they won't be looked at, so it doesn't matter.
        /\ m.mterm = currentTerm[i]
        ...


Моделировать отправку кому-то одному - очень легко. Например, в Raft-e явно указывается кому и от кого пришло сообщение.

Удалять сообщения из этого множества ненужно, так как в реальном мире сообщения могут дублироваться или никогда не доставляться. Конструкция с \E полностью удовлетворяет этому поведению, так как некоторые сообщения могут вообще никогда не обработаться, а некоторые обработаться несколько раз. Если же удалять сообщения после обработки из сета, то мы отказываемся от дедубликации и потери сообщений. Нам надо самим добавить экшены, которые будут моделировать такие ситуации.

Так сделали в Raft

    DuplicateMessage(m) ==
        /\ Send(m)
    /\ UNCHANGED <<serverVars, candidateVars, leaderVars, logVars>>

    DropMessage(m) ==
        /\ Discard(m)
    /\ UNCHANGED <<serverVars, candidateVars, leaderVars, logVars>>

В распределенных алгоритмов почти никогда не (дописать). Большинство распределенных алгоритмов разбито на фазы (ABD, Paxos, Raft, ...), переход между фазами происходит при достижении кворума. Кворумы используются для маскировки сбоев: операция успешно завершается, даже если не все узлы системы отвечают. Так что почти всегда узлы в распределенном алгоритме отправляют сообщение не отдельным учестникам, а всем узлы и дожидаются подтверждения/ответа от произвольного кворума.

Чтобы моделировать отправку сообщений всем участникам система и сэкономить на состояниях графа конфигураций, можно, например, не указывать явных адресатов.

например в Paxos, сообщения от proposer-ов отправляются всем acceptor-ам сразу. В них не указывается адресат:

    /\ Send([type |-> "1a", bal |-> b])

Таким образом, нода отправляет сообщение всем остальным участникам.

Но важно, что в сообщениях, которые приходят от acceptor-а к proposer-у надо указывать адресат, так как proposer должен знать, когда он наберет кворум ответов.

    /\ Send([type |-> "1b", acc |-> a, bal |-> m.bal,
             mbal |-> maxVBal[a], mval |-> maxVal[a]])

Такие техники упрощают алгоритм по сравнению с реальным миром и уменьшают гранулярность событий, так что существует вероятность потерять какие-то сложные сценарии с гонками и надо быть аккурантым.

### Сбои
Существуют 3 основные модели сбоев:
* Отказ - узел взрывается и больше никогда не принимает участия в системе.
* Рестарт - узел не отвечал некоторое время, а потом опять начал участвовать в системе.
* Византийский - узел можно отходить от протокола общения.

С моделирование отказов все просто. Так как мы выбрали асинхронную систему, у нас нет возможности отличить умершую ноду и ту, которая долго обрабатывает наш запрос.

Авторы FLP пишут в [статье](https://groups.csail.mit.edu/tds/papers/Lynch/jacm85.pdf), что они не различают умершую ноду от той, которая долго не отвечает. В рамках графа конфигураций в пути просто не будет состояний, связанных с умершим узлом. В TLA+ аналогично model checker исследует такие состояния, где узел модет не отвечать. 

Поддержка рестртов требует от алгоритма хранить часть своего состояния в персистентом хранилища.

Например, в RAFT реплика должна отдавать не более одного голоса в пределах одного терма, в противном случае ...  Чтобы гарантировать это, перед ответом на RequestVote реплика должна

В Basic Paxos acceptor после ответа на сообщение в первой фазе, не должен реагировать на сообщения с меньшим ballot number. Чтобы гарантировать это, acceptor должен узнать, какой ballot number он у себя успел сохранить.

В спеке Basic Paxos рестарты не моделируются, а в Raft-е есть отдельный экшен

    \* Server i restarts from stable storage.
    \* It loses everything but its currentTerm, votedFor, and log.
    Restart(i) ==
        /\ state'          = [state EXCEPT ![i] = Follower]
        /\ votesResponded' = [votesResponded EXCEPT ![i] = {}]
        /\ votesGranted'
        /\ voterLog'
        /\ nextIndex'
        /\ matchIndex'
        /\ commitIndex'
        /\ UNCHANGED <<messages, currentTerm, votedFor, log, elections>>


Самое интересное тут UNCHANGED переменные, которые во время экшена не изменяются. Среди них есть votedFor - это кандидат за которого сервер проголосовал в текущем раунде. Именно это должно храниться в персистентном хранилище, что бы нода после рестарта знала, за кого она отдала голос

Византийский отказы - ...

### Недетерминизм
Недетерминизм является одним из главных источников сложности проектирования распределенных систем. Недетерминизм возникает естественным образом из-за реактивной природы распределенных систем: они реагируют на внешние воздействия (запросы клиентов) и подвержены влиянию неконтролируемой среды (сеть, течение времени), железа / runtime-а  (паузы gc) и сбоев.

Примером недетерминизма в многопоточных программах может служить поведение планировщика ОС. Заранее неизвестно, когда планировщик будет производить переключение потоков и порядок на них. Из-за этого тестирование становится очень сложным. В распределенных системах недетерминизм заключается во времени доставки сообщений, их потери или дубликации, в сбоях и рестартах нод системы, в запросах клиентов, которые будут поступать. Так же выборы лидера, которые используется в многих алгоритмах, играют важную роль в недетерминизме, так как заранее нельзя сказать, какая последовательность нод будет выбрана.

Какие есть способы выразить недетерминизм в TLA?
* Одним из способов описать недетерминизм системы - это использование дизъюнкции в экшенах. Например, у нас клиент может отправить SELECT или INSERT на ноду БД, тогда мы можем в Next добавить дизъюнкцию этих двух событий (Next == Select \/ Insert). Никакой гарантии на порядок взятия экшенов нет, а значит model checker должен проверить все возможные порядки на этих дейсвтиях.
* Еще один способ моделировать недетерминизм - это оператор \E. он используется для того, чтобы выбрать, какой из узлов системы следующим сделает “ход” - получит сообщение, перезагрузится и т.п.

Примеры:

В спеке Raft-а выбирается какое сообщение будет доставлено или какой из узлов выполнит действие

    Next == /\ \/ \E i \in Server : Restart(i)
               \/ \E i \in Server : Timeout(i)
               \/ \E i,j \in Server : RequestVote(i, j)
               \/ \E i \in Server : BecomeLeader(i)
               \/ \E i \in Server, v \in Value : ClientRequest(i, v)
               \/ \E i \in Server : AdvanceCommitIndex(i)
               \/ \E i,j \in Server : AppendEntries(i, j)
               \/ \E m \in DOMAIN messages : Receive(m)
               \/ \E m \in DOMAIN messages : DuplicateMessage(m)
               \/ \E m \in DOMAIN messages : DropMessage(m)


В спеке Paxos-а выбирается какое сообщение будет обработано или какой из acceptor-ов ответит на запрос

    Phase2b(a) == \E m \in msgs : /\ m.type = "2a"
                                  /\ m.bal \geq maxBal[a]
                                  /\ maxBal' = [maxBal EXCEPT ![a] = m.bal]
                                  /\ maxVBal' = [maxVBal EXCEPT ![a] = m.bal]
                                  /\ maxVal' = [maxVal EXCEPT ![a] = m.val]
                                  /\ Send([type |-> "2b", acc |-> a,
                                           bal |-> m.bal, val |-> m.val])

    Next == \/ \E b \in Ballot : \/ Phase1a(b)
                                \/ \E v \in Value : Phase2a(b, v)
           \/ \E a \in Acceptor : Phase1b(a) \/ Phase2b(a)

В Kafka-е:

    ControllerShrinkIsr == \E replica \in Replicas :
        /\  \/  /\ quorumState.leader = replica
                /\ quorumState.isr = {replica}
                /\ ControllerUpdateIsr(None, quorumState.isr)
            \/  /\ quorumState.leader = replica
                /\ quorumState.isr # {replica}
                /\ ControllerUpdateIsr(None, quorumState.isr \ {replica})
            \/  /\ quorumState.leader # replica
                /\ replica \in quorumState.isr
                /\ quorumState.isr # {replica}
                /\ ControllerUpdateIsr(quorumState.leader, quorumState.isr \ {replica})
        /\ UNCHANGED <<nextRecordId, replicaLog, replicaState>>

В одном экшене все действия происходят за один раз

Например:

В экшене для acceptor-а в первой фазе алгоритма Паксоса

    Phase1b(a) == /\ \E m \in msgs :
                      /\ m.type = "1a"
                      /\ m.bal > maxBal[a]
                      /\ maxBal' = [maxBal EXCEPT ![a] = m.bal]
                      /\ Send([type |-> "1b", acc |-> a, bal |-> m.bal,
                                mbal |-> maxVBal[a], mval |-> maxVal[a]])
                  /\ UNCHANGED <<maxVBal, maxVal>>

acceptor получает сообщение от proposer-а. Проверяет, что ballot у сообщения больше, чем текущий локальный у acceptor-а, и если это так, то он обновляет максимально полученный ballot и отвечает текущему proposer-у.

В этом экшене инкапсулированны 3 дейтсвия, которые произошли в системе.

Можно провести аналогия с FLP. В док-ве авторы в переходе между состояниями объединяют 3 действия: получение сообщения, изменение состояния и отправку сообщения. Так как граф конфигураций в FLP и в TLA+ однотипны, то можно поступить аналогично и в нашей спеке

В экшене TLA происходит все аналогично. Надо очень аккуратно выбрать действия, которые мы будем делать атомарно в спеке. Для распределенных систем это сделать проще, так как почти все внутренние действия ноды для изменения состояния рассматриваются до отправки нового сообщения, и для других участников системы это происходит как будто атомарно.

Для параллельных алгоритмов все сложнее, потому что, если использовать в PlusCal-е однин лэйбл (это те действия, которые происходят атомарно) можно потерять гонки, которые происходят в алгоритме, так как мы можем написать в лэбле чтение из одной ячейки и запись в другую.

### Cвойства системы
Требование к системе/алгоритму мы формулируем в виде свойств. Свойство - это набор траекторий в графе состояний.

Будем рассматривать 2 вида свойста для распредедленных систем:
* Safety свойство говорит о том, что если свойство нарушено на траектории, то существует перфикс на котором это свойство нарушается и на любом расширении префикса свойство будет нарушено. Эти свойство должны выполняться в каждом состоянии нашей системы.
* Liveness свойство говорит о том, что для любого бесконечного поведение существует состояние, в котором свойство выполняется.

При верификации распределенных алгоритмов достаточно проверять только свойства двух приведенных типов: потому что по теореме https://pron.github.io/posts/tlaplus_part3#safety-and-liveness любое наше св-ва можно выразить через safety и liveness.

Приведем примеры safety свойств:

В Raft инвариантом является то, что во время в каждом раунде только один лидер. В TLA+ это выражено safety свойством

    BothLeader( i, j ) ==
         /\ i /= j
         /\ currentTerm[i] = currentTerm[j]
         /\ state[i] = Leader
         /\ state[j] = Leader

    MoreThanOneLeader ==
        \E i, j \in Server :  BothLeader( i, j )

В Paxos главным свойством является то, что если в каком-то из раундов было выбрано значение v с proposal number b, то для всех сообщений propose(v', p'), где p' > p => v' = v. В TLA+ оно выражено safety свойством:

    Agreed(v,b) == \E Q \in Quorum2: \A a \in Q: Sent2b(a,v,b)
    NoFutureProposal(v,b) == \A v2 \in Value: \A b2 \in Ballot: (b2 > b /\ Sent2a(v2,b2)) => v=v2

    SafeValue == \A v \in Value: \A b \in Ballot: Agreed(v,b) => NoFutureProposal(v,b)

В Kafka одним из свойств является то, что записи, которые подтверждены должны находится на всех репликах в кворуме. Это выражено safety свойством в TLA+:

    StrongIsr == \A r1 \in Replicas :
        \/ ~ ReplicaPresumesLeadership(r1)
        \/ LET  hw == replicaState[r1].hw
           IN   \/ hw = 0
                \/ \A r2 \in quorumState.isr, offset \in 0 .. (hw - 1) : \E record \in LogRecords :
                    /\ ReplicaLog!HasEntry(r1, record, offset)
                    /\ ReplicaLog!HasEntry(r2, record, offset)

Termination является одним из liveness свойств, но для распределенных систем мы не можем говорить о прекращении работы из-за свойства реактивности.

Liveness свойства пости никогда не применяются для распределенных систем, так как о завершении можно говорить, только в случае частично синхронной или синхронной сети, а в TLA+ мы не моделируем их.

### Проврека моделей согласованности
Если мы смотрим на систему не изнутри, а снаружи, то все алгоритмы от нас скрыты, нам доступно лишь наблюдаемое поведение, кооторое формулизуется в виде моделей согласованности.

В болшинстве систем доступны 2 модели согласованности: eventual consistency и linearizability. Мы рассматриваем именно linearizability, так как оно наиболее понятно пользователю описывает поведение системы и является одной из самых строгих гарантий.

Для описания таких свойста надо иметь историю в каком-нибудь виде, так как они основываются на порядке выполнения операций.

В TLA+ для этого логично использовать tuple, куда надо сохранять события, которые произошли в системе (например действия транзакций, начало и конец вставок в DataStore и т.д.). С помощью этой служебной переменной мы сможем проверять интересующие нас свойства. Для работы с историей будет удобно написать операторы, чтобы работать с определенным типом событий.

    ActiveOrFinalizedTxns(h) == {e.txnid : e \in Range(h)}        (* all transactions apart from those that have not yet started *)
    NotYetStartedTxns(h)     == TxnId \ ActiveOrFinalizedTxns(h)
    CommittedTxns(h)         == {e.txnid : e \in SelectEvents(h, LAMBDA e : e.op \in {"commit"})}
    AbortedTxns(h)           == {e.txnid : e \in SelectEvents(h, LAMBDA e : e.op \in {"abort"})}
    FinalizedTxns(h)         == CommittedTxns(h) \union AbortedTxns(h)
    ActiveTxns(h)            == ActiveOrFinalizedTxns(h) \ FinalizedTxns(h)

Например: алгоритм SI гарантирует Serializability. Для этого мы должны проверить, что конкурирующие транзакции можно упорядочить (они не образуют цикл) и не нарушаются инварианты алгоритма на запись и чтение. Например, что если две транзакции конкурируют, а их write set-ы пересекаются, то применится только одна из них у которой временная метка меньше, а другая будет отменена.

    BernsteinSerializable(h) ==  ~ IsCycle(BernsteinMVSG(h))

    SemanticsOfSnapshotIsolation ==
        /\ CorrectReadView
        /\ FirstCommitterWins

Заметим, что можно явно не моделировать историю событий. Например в Paxos историю можно восстановить из множества сообщений, которые находятся в системе, используя Ballot и фазу у сообщений.


### Обрезки

Когда мы пишем распределенную систему нам удобно рассуждать с точки зрения наблюдателя. Моделировать компоненты, которые играют важную роль в нашей системе (ноды, клиент и e.t.c). Описать систему, как ее видит польщователь, так ка это даст наибольшую интуитивность.

Например:

В Paxos есть отдельные сущности для acceptor-ов, системы кворумов, которые играют важную роль в алгоритме.

    CONSTANT Value, Acceptor, Quorum1, Quorum2

Действия proposer-ов моделируют отдельными экшенами, которые предлагают значения.

В Raft явно моделируются узлы, которые участвуют в алгоритме:

    \* The set of server IDs
    CONSTANTS Server

Аналогично происходит в Kafka-е


Можно моделировать не участников системы, а описать поведение объектов, с которыми эта система работает (описать как будут себя вести компоненты сервиса). Пусть у нас есть лог с записями. Можем моделировать не клиента, который эти записи делает, а сами записи изнутри, то есть описать как записи взаимодействуют друг с другом.

Такой подход используется в спеке SI. Там моделируется конечное число транзакций, которые поступают в систему.

И для них описываются их основные действия. Начало, конец, аборт и т.п.

    BeginEventsT   == [op : {"begin"},  txnid : TxnId]
    AbortEventsT   == [op : {"abort"},  txnid : TxnId, reason : AbortReasons]
    CommitEventsT  == [op : {"commit"}, txnid : TxnId]
    ReadEventsT    == [op : {"read"},   txnid : TxnId, key : Key, ver : TxnId]
    WriteEventsT   == [op : {"write"},  txnid : TxnId, key : Key]

Вот, что Лампорт пишет об этом:
"Mathematical manipulation of specifications can yield new insight. A producer/consumer system can be written as the conjunction of two formulas, representing the producer and consumer processes. Simple mathematics allows us to rewrite the same specification as the conjunction of n formulas, each representing a single buffer element. We can view the system not only as the composition of a producer and a consumer process, but also as the composition of n buffer-element processes. Processes are not fundamental components of a system, but abstractions that we impose on it. This insight could not have come from writing specifications in a language whose basic component is the process."

Это все сильно завязано на свойства, которые мы хотим проверять. Нам не интересно моделировать записи в логе, когда мы хотим проверить, например, алгоритм для выбора лидера в системе. Или репликацию между несколькими серверами. Так как со со "стороны записей" нет понимания о репликации. Нам хочется описывать сущности с которыми работает система, когда надо проверить порядок на них, или увидеть как они взаимодействуют между собой.

Примеры:
* Acceptor-ы и Proposer-ы в Paxos
* Leader и follwer-ы в Raft и Kafka

 и обмениваются сообщениями

Примеры:
* Запрос на участие в выборах лидера в Kafka-е
* Запросы Promise и Accept в Paxos
* Запросы во время выборов лидера и репликацию в Raft-е

-------------------

Каждая траектория в спеке - это бесконечный набор состояний системы, но это не значит, что она не пригодна для model checker-а. В основе TLC лежит обход в ширину графа состояний, из-за этого чекеру не страшно, что траектории бесконечные, так как они из себя представляют циклы разной длины, потому что множество состояний системы конечно.

------------------

В большинстве спек темпоральность TLA+ проявляется в виде `[]` перед экшеном Next
