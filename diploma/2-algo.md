# Часть 2 (Алгоритмы)

## New
### Словарь
* Нода - 1 сервер, на который приходят запросы пользователя/который участвует в алгоритме
* Участники - все ноды системы, клиент.
* Чекер - TLC

### Intro
Есть синхронная модель сети и асинхронная. В первой есть гарантии на время доставки сообщений и на скорость работы компонент системы. TLA+ использует асинхронную модель сети. Эта модель подразумевает, что в системе:
    * нет ограничений на время доставки сообщений
    * нет ограничений на дрейф часов
    * нет ограничений на относительную скорость работы процессов
Используют именно ее, так как лучше всего описывает настоящий мир. Но все же мы сильно пессимизируем, когда описываем систему в этой модели, так как в реальности сеть не бывает бесконечно долгой. Это валидно делать, так как система, работающая в таких суровых ограничениях, будет работать и в реальном мире.

Так же мы говорим, что узлы в нашей системе могут выходить из строя, а потом снова начать работать или отказать навсегда. Мы не рассматриваем Византийские отказы, так как мы проверяем устойчивость и правильность  алгоритма или системы в целом и не рассматриваем внешние вмешательства (кроме сети).

### Уровень абстракции
Для пользователя распределенная система - это некоторый сетевой адрес, по которому отправляются команды (Get/Set для k/v хранилища, Create/Append/Delete для файловых систем и т.п.) и ответ, чаще всего через некоторую клиентскую библиотеку или простой crud, так что для пользователя такая система - конкурентный / многопоточный объект, а сетевой адрес - это имя этого объекта. Пользователь работает в модели shared memory.

В то же время изнутри система представляет собой отдельные микросервисы / узлы, которые играют разные роли, т.е. узлы самой системы живут в модели message passing

Примеры:
* Acceptor-ы и Proposer-ы в Paxos
* Leader и follwer-ы в Raft и Kafka

 и обмениваются сообщениями

Примеры:
* Запрос на участие в выборах лидера в Kafka-е
* Запросы Promise и Accept в Paxos
* Запросы во время выборов лидера и репликацию в Raft-е

Все это справедливо не только для самой системы, но и для отдельных ее компонентов. Надо уметь понимать, что нужно описывать в спеке, а что нет. Для начала надо выделить основных участников системы. Ими могут быть ноды (Паксос, Raft) или сущности, над которыми происходит воздействие(транзакции в SI). То есть, можно описать систему со стороны участников, которые производят действия или со стороны объектов, над которыми эти действия выполняются.

Когда мы пишем распределенную систему нам удобно рассуждать с точки зрения наблюдателя. Моделировать компоненты, которые играют важную роль в нашей системе (ноды, клиент и e.t.c). Описать систему, как ее видит польщователь, так ка это даст наибольшую интуитивность

Можно моделировать не участников системы, а описать поведение объектов, с которыми эта система работает (описать как будут себя вести компоненты сервиса). Пусть у нас есть лог с записями. Можем моделировать не клиента, который эти записи делает, а сами записи изнутри, то есть описать как записи взаимодействуют друг с другом. Такой подход используется в спеке SI. Там моделируется конечное число транзакций, которые поступают в систему. И для них описываются их основные экшены. Начало, конец или аборт.

Вот, что Лампорт пишет об этом:
"Mathematical manipulation of specifications can yield new insight. A producer/consumer system can be written as the conjunction of two formulas, representing the producer and consumer processes. Simple mathematics allows us to rewrite the same specification as the conjunction of n formulas, each representing a single buffer element. We can view the system not only as the composition of a producer and a consumer process, but also as the composition of n buffer-element processes. Processes are not fundamental components of a system, but abstractions that we impose on it. This insight could not have come from writing specifications in a language whose basic component is the process."

Это все сильно завязано на св-ва, которые мы хотим проверять. Нам не интересно моделировать записи в логе, когда мы хотим проверить, например, алгоритм для выбора лидера в системе. Или репликацию между несколькими серверами. Так как со со "стороны записей" нет понимания о репликации. Нам хочется описывать сущности с которыми работает система, когда надо проверить порядок на них, или увидеть как они взаимодействуют между собой.

Сравним существующие системы и их спеки:
* Paxos/Raft: В этих алгоритмах основными участниками являются ноды. Мы хотим описать алгоритм общения между ними, моделируя передачу сообщений, сеть и т.д. Это самый высокий уровень детализации. Мы хотим полностью описать, как ведут себя участники алгоритма, их протокол взаимодействия и проблемы, с которыми они сталкиваются. В системах, где одной из составляющих является обмен сообщениями по сети надо уметь моделировать асинхронность сети.
* Kafka: Участники репликации в Kafke используют  ZooKeeper. Мы не хотим моделировать, так как Zookeeper - это отказоустойчивое глобальное разделяемое состояние, которое наблюдают все участники. ZNode - это распределенная отказоустойчивая ячейка памяти, которую изменяют наши участники. Можно провести аналогию между ZNode в ZooKeeper-е и атомиком в C++. Создатели сами проводят такое сравнение: "Our system, Zookeeper, hence implements an API that manipulates simple wait-free data objects organized hierarchically as in file systems", так как wait-free  - это св-во конкурентных объектов. Вот, что сам автор спеки пишет по этому поводу: "This is the model's equivalent of the state in Zookeeper, but generally we ignore the complexity of Zookeeper itself. Instead we allow simple atomic operation to the state directly within individual actions". Поэтому, это просто variable и в спеке участники будут ее изменять в экшенах. Единственное место, где мы хотим описать взаимодействие реплик - это обмен сообщениями о leader election.
* Percolator: Алгоритм SI поверх BigTable. В этой спеке главными участниками являются клиенты. Вся метаинформация находится в хранилище. Считаем, что это отказоустойчивые распределенные ячейки памяти (аналогично было в Kafka-е). Обмена сообщениями по сети нет, поэтому нам не надо ее моделировать. Эта спека менее детализирована, чем все остальные. Но для проверки св-в это не так важно.

Для каждой системы, которую надо описать, в самом начале надо понять, какой уровень детализации нужно выбрать для разных. компонент.

TLA+ использует язык LTL. Это язык линейного времени. Последовательность описывает одно исполнение нашей системы. Есть другая логика - CTL. Где рассматривается не одно исполнение, а целое дерево. В нем утверждения формулируются о нескольких исполнениях сразу, а в LTL мы формулируем св-во для одного какого-то исполнения. Была выбрана эта модель, так как нас интересуют высказывания про отдельные исполнения, а не про несколько сразу, так как св-в таких не возникает в распределенных системах.

Для описания св-в используются темпоральные операторы, но среди них нет оператора next, так как нельзя говорить о следующем действии при абстрактном описании. При использовании этого оператора убираются шаги заикания, а то, что вы хотели описать можно задать с помощью темпоральной логики без оператора next, который повысит сложность темпоралной логики.

### Сбои
Сложность распределенных систем в первую очередь обусловлена необходимостью скрывать сбои от пользователя, так что надо уметь описывать проблемы, которые могут происходить. Большинство распределенных алгоритмов и систем содержит несколько нод, сервера не могут жить вечно, могут долго не отвечать и т.д. В асинхронной системе у нас нет возможности отличить умершую ноду и ту, которая долго обрабатывает наш запрос. В связи с этим, возникает вопрос: Надо ли нам моделировать падение сервера в нашей спеке?

Первым делом надо понять, что такое умершая нода в рамках графа конфигураций.
Мы описываем состояния, в которых наша система может прибывать, и переходы между ними. Описание системы переводится в граф конфигураций (структуре Крипке), который потом исследует TLC. Получается граф, где вершины - это состояние нашей системы, а ребра - это переходы между ними (доставка сообщения или срабатывания таймаута). Аналогичную конструкцию используют в док-ве [FLP](https://groups.csail.mit.edu/tds/papers/Lynch/jacm85.pdf). Авторы FLP пишут, что они не различают умершую ноду от той, которая долго не отвечает. В рамках графа конфигураций в пути просто не будет переходов, связанных с умершей нодой.

Что же такое отказ ноды в рамках TLA? Model checker исследует все бесконечные поведения в этом графе (при условии, что граф конечен) в том числе и те, в которых некоторые процессы не совершают “ходов” (не реагируют на сообщения и не отправляют их). В траектории просто не будет перехода, который задействует мертвую ноду. Значит, можно считать, что ситуация аналогично FLP, и в TLA можно не моделировать сбои узлов.

Так же в алгоритм можно добавить поддержку рестартов, при наличии персистентного хранилища. Например, если в Paxos-е моделировать рестарты, то важно, чтобы acceptor, который подтвердил на второй фазе значения одного из proposer-ов, после рестарта узнал об этом, а не участвовал в новом раунде, так как proposer будет "думать", что рестартонувший acceptor все еще знает о его значении. Или в Raft-е во время выборов лидера важно не потерять голос от одного из участника.

В спеке Basic Paxos рестарты не моделируются, а в Raft-е есть отдельный [экшен](https://github.com/ongardie/raft.tla/blob/34cdd49d22615426ea00a6605b95be57b3cab49a/raft.tla#L166) для этого. Самое интересное тут UNCHANGED переменные, которые во время экшена не изменяются. Среди них есть votedFor - это кандидат за которого сервер проголосовал в текущем раунде. Именно это должно храниться в персистентном хранилище, что бы нода после рестарта знала, за кого она отдала голос

### Моделирование сети и отправки сообщений
При моделировании обмена сообщениями в нашей системе появляется еще один важный участник - это сама сеть. Сеть может породить проблемы при передаче сообщений между нодами (дубликацию, пропажу и т.д.). Все системы в TLA моделируется в предположении, что сеть асинхронная, так как это наиболее похожая на внешний мир система. Асинхронность обозначает, что нельзя говорить о времени на доставку сообщений аналогично в реальном мире ноды могут из-за внешних факторов не отвечать, или долго обрабатывать сообщения.

Во-перавых, надо уметь моделировать все сообщения, которые есть в нашей системе (находятся в "проводах" или доставляются до ноды). Для асинхронности в TLA есть тип данных - set, он помогает обеспечить недетерминизм в системе. Конструкция \E val \in set: P(val). Выдает произвольный элемент мн-ва, который удовлетворяет предикату. Тем самым порядок на получение элемента не гарантирован. Это помогает моделировать reordering на отправленных сообщениях и их задержку. Например, в [Raft-е](https://github.com/ongardie/raft.tla/blob/34cdd49d22615426ea00a6605b95be57b3cab49a/raft.tla#L306) при получении сообщения проверяется в каком term-е оно было отправлено, чтобы не обрабатывать сообщения из прошлого. Моделировать отправку кому-то одному - очень легко. Например, в [Raft-e](https://github.com/ongardie/raft.tla/blob/34cdd49d22615426ea00a6605b95be57b3cab49a/raft.tla#L306) явно указывается кому и от кого пришло сообщения. Удалять сообщения из этого мн-ва ненужно, так как в реальном мире сообщения могут дублироваться или никогда не доставляться. Конструкция с \E полностью удовлетворяет этому поведению, так как некоторые сообщения могут вообще никогда не обработаться, а некоторые обработаться несколько раз. Если же удалять сообщения после обработки из сета, то мы отказываемся от дедубликации и потери сообщений. Нам надо самим добавить экшены, которые будут моделировать такие ситуации. Так сделали в [Raft](https://github.com/ongardie/raft.tla/blob/34cdd49d22615426ea00a6605b95be57b3cab49a/raft.tla#L438)

Большинство распределенных алгоритмов разбито на фазы (ABD, Paxos, Raft, ...), переход между фазами происходит при достижении кворума. Кворумы используются для маскировки сбоев: операция успешно завершается, даже если не все узлы системы отвечают. Так что почти всегда нам нужно отправлять не просто отдельные сообщения, а отправить одну команду на все узлы и дождаться подтверждения/ответа от произвольного кворума. Чтобы эмитировать отправку сообщений всем участникам система и сэкономить на состояниях графа конфигураций, можно, например, как в Paxos у сообщений не указывать явных адресатов. Таким образом, нода отправляет сообщение всем остальным участникам. Но важно, что в сообщениях, которые приходят от acceptor-а к proposer-у надо указывать адресат, так как proposer должен знать, когда он наберет кворум ответов.

С помощью мн-ва из сообщений можно моделировать liveness св-ва, как это сделано в [Paxos](https://github.com/fpaxos/fpaxos-tlaplus/blob/c562667ad96bcb9e07a30417a45b49c5d21d1fbe/FPaxos.tla#L89). Проверяется, что принято ровно одно значение.

### Недетерминизм
Недетерминизм является одним из главных источников сложности проектирования распределенных систем. Недетерминизм возникает естественным образом из-за реактивной природы распределенных систем: они реагируют на внешние воздействия (запросы клиентов) и подвержены влиянию неконтролируемой среды (сеть, течение времени), железа / runtime-а  (паузы gc) и сбоев.

Примером недетерминизма в многопоточных программах может служить поведение планировщика ОС. Заранее неизвестно, когда планировщик будет производить переключение потоков и порядок на них. Из-за этого тестирование становится очень сложным. В распределенных системах недетерминизм заключается во времени доставки сообщений, их потери или дубликации, в сбоях и рестартах нод системы, в запросах клиентов, которые будут поступать. Так же выборы лидера, которые используется в многих алгоритмах, играют важную роль в недетерминизме, так как заранее нельзя сказать, какая последовательность нод будет выбрана.

Какие есть способы выразить недетерминизм в TLA?
* Одним из способов создать недетерминизм системы - это использование дизъюнкции в экшенах. Например, у нас клиент может отправить SELECT или INSERT на ноду БД, тогда мы можем в Next добавить дизъюнкцию этих двух событий (Next == Select \/ Insert). Никакой гарантии на порядок взятия экшенов нет, следовательно, будет выбран произвольный элемент.
* Еще один способ моделировать недетерминизм - это оператор \E. Он используется для того, чтобы выбрать произвольный элемент из мн-ва. Этот способ используется, например, во время моделирования доставки сообщения: мы выбираем какое-то из сообщений, которые есть сейчас в системе ([Raft](https://github.com/ongardie/raft.tla/blob/34cdd49d22615426ea00a6605b95be57b3cab49a/raft.tla#L459), [Paxos](https://github.com/fpaxos/fpaxos-tlaplus/blob/c562667ad96bcb9e07a30417a45b49c5d21d1fbe/FPaxos.tla#L63)). В [Kafka](https://github.com/hachikuji/kafka-specification/blob/3cc3cf6914f76573f8b66fb700f8b90ac7ca8bed/KafkaReplication.tla#L158) аналогичный подход используется еще и для того, чтобы выбрать реплику, которая будет выполнять действие.

Так же одним из важных моментов в том, что в одном экшене все действия происходят за раз. Можно провести аналогия с FLP. В док-ве авторы в переходе между состояниями объединяют 3 действия: получение сообщения, изменение состояния и отправку сообщения. В экшене TLA происходит все аналогично. Надо очень аккуратно выбрать действия, которые мы будем делать атомарно в спеке. Для распределенных систем это сделать проще, так как мы почти все внутренние действия ноды для изменения состояния рассматриваем до отправки нового сообщения, и для других участников системы это происходит как будто атомарно. Для параллельных алгоритмов все сложнее, потому что, если использовать PlusCal в однин лэйбл (это те действия, которые происходят атомарно) можно потерять гонки, которые происходят в алгоритме, так как мы можем написать в лэбле чтение из одной ячейки и запись в другую.
