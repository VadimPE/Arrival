# Часть 2 (Алгоритмы)
## New
### Словарь
* Узел - 1 сервер, на который приходят запросы пользователя/который участвует в алгоритме
* Реплика - сервер с копией данных нашего хранилища
* Участники - все узлы системы, клиент.
* Чекер - TLC
* Выполнение - бесконечная последовательность состояний системы. (Трасса/траектория)

### Реактивность
У распределенной системы нет точки входа и выхода. Она реагирует на действия клиентов на протяжении всей своей работы.

Мы можем описать ее поведение набором состояний, которые описывают как система обрабатывает тот или иной запрос. Это соотвествует вершинам в грфе конфигураций

Система меняется, когда какой-то узел реагирует на доставленное сообщение от других узлов. Это соответсвует переходам в графе конфигураций.

Реакция узлов выражена в виде экшенов, которые отвечают за получение сообщения, изменения состояния системы и отправки нового сообщения.

Например:
* BecomeLeader - узле победил leader election
* Restart - узел произвел рестарт
* ClientRequest - в систему отправлен запрос от клиента
* RequestVote - узел отправил сообщение
* HandleRequestVoteResponse -  узел получил сообщение
* DuplicateMessage - сеть потеряла сообщение

### Уровень абстракции
Распределенная система представляет собой отдельные узлы / микросервисы, которые играют разные роли, и взаимодействуют с помощью модели обмена сообщениями. C другой стороны от пользователя скрыты все подробности устройства системы и для него она - это некоторый сетевой адрес, по которому отправляются команды (Get/Set для k/v хранилища, Create/Append/Delete для файловых систем и т.п.) и ответ, чаще всего через некоторую клиентскую библиотеку или простой crud. Для пользователя система – это конкурентный объект в модели shared memory, а сетевой адрес - по сути имя этого объекта.

Все рассуждения выше можно применить не только с самой системе, но и к отдельным ее компонентам. Например: Kafka использует ZooKeeper для хранения состояния кворума. Для узлов, которые играют роль контроллера, это разделяемое состояние в shared memory, при этом внутри ZooKeeper - это несколько узлов, которые обмениваются сообщениями.

Сравним существующие системы и их спеки:

Paxos/Raft: В спеках этих алгоритмов выбран самый высокий уровень детализации: моделируется состояние отдельных узлов распределенной системы и протокол их взаимодействия, моделируется отказ некоторых узлов. В системах, где одной из составляющих является обмен сообщениями по сети надо уметь моделировать асинхронность сети.

Kafka: В этой системе лидер принимает новые записи клиента и отправлчет их остальным узлам в кворуме. Контроллер используют Zookeeper для упорядочивания апдейтов кворума. Для узлов, которые выполняют роль контроллера Zookeeper - это отказоустойчивое глобальное разделяемое состояние, которое наблюдают все участники, для него znode - это распределенная отказоустойчивая ячейка памяти, которую изменяют узлы. Можно провести аналогию между ZNode в ZooKeeper-е и атомиком в C++. Создатели сами проводят такое сравнение: "Our system, Zookeeper, hence implements an API that manipulates simple wait-free data objects organized hierarchically as in file systems", так как wait-free  - это свойство конкурентных объектов. Вот, что сам автор спеки пишет по этому поводу: "This is the model's equivalent of the state in Zookeeper, but generally we ignore the complexity of Zookeeper itself. Instead we allow simple atomic operation to the state directly within individual actions". Поэтому, это просто variable и в спеке участники будут ее изменять в экшенах. Единственное место, где мы хотим описать взаимодействие реплик - это обмен сообщениями во время вставки, так как она подтвердится клиенту, только когда все реплики из ISR ее применят к себе.

Percolator - это протокол ckient side расаределенрныз транзакций над BigTable. Big Table - это распределденное отказоустойчивое хранилище. Все взаимодействие между клиентами происхрдит с помощью Big Table. В этой спеке самый высокий уровень абстракции, в котором нет никакого взаимодействия по сети между узлами системы.

Первый шаг работы над спецификаций системы -  выбор уровень детализации разных компонент.

Следует выделить основных участников системы. Ими могут быть узлы (Паксос, Raft) или сущности, над которыми происходит воздействие(транзакции в SI).

В самом начале надо понять: какие компоненты системы надо моделировать, а какие нет. Например в ClickHouse мы не моделируем log в ZooKeeper, а считаем, что это просто атомарная ячейка памяти. А в Raft log явно представлен на всех репликах.

### Моделирование участников
Когда мы пишем распределенную систему нам удобно рассуждать с точки зрения наблюдателя. Моделировать компоненты, которые играют важную роль в нашей системе (узлы, клиент и e.t.c). Описать систему, как ее видит польщователь, так ка это даст наибольшую интуитивность.

Например:

В Paxos есть отдельные сущности для acceptor-ов, системы кворумов, которые играют важную роль в алгоритме.

    CONSTANT Value, Acceptor, Quorum1, Quorum2

Действия proposer-ов моделируют отдельными экшенами, которые предлагают значения.

В Raft явно моделируются узлы, которые участвуют в алгоритме:

    \* The set of server IDs
    CONSTANTS Server

Аналогично происходит в Kafka-е


Можно моделировать не участников системы, а описать поведение объектов, с которыми эта система работает (описать как будут себя вести компоненты сервиса). Пусть у нас есть лог с записями. Можем моделировать не клиента, который эти записи делает, а сами записи изнутри, то есть описать как записи взаимодействуют друг с другом.

Такой подход используется в спеке SI. Там моделируется конечное число транзакций, которые поступают в систему.

И для них описываются их основные действия. Начало, конец, аборт и т.п.

    BeginEventsT   == [op : {"begin"},  txnid : TxnId]
    AbortEventsT   == [op : {"abort"},  txnid : TxnId, reason : AbortReasons]
    CommitEventsT  == [op : {"commit"}, txnid : TxnId]
    ReadEventsT    == [op : {"read"},   txnid : TxnId, key : Key, ver : TxnId]
    WriteEventsT   == [op : {"write"},  txnid : TxnId, key : Key]

Вот, что Лампорт пишет об этом:
"Mathematical manipulation of specifications can yield new insight. A producer/consumer system can be written as the conjunction of two formulas, representing the producer and consumer processes. Simple mathematics allows us to rewrite the same specification as the conjunction of n formulas, each representing a single buffer element. We can view the system not only as the composition of a producer and a consumer process, but also as the composition of n buffer-element processes. Processes are not fundamental components of a system, but abstractions that we impose on it. This insight could not have come from writing specifications in a language whose basic component is the process."

Это все сильно завязано на свойства, которые мы хотим проверять. Нам не интересно моделировать записи в логе, когда мы хотим проверить, например, алгоритм для выбора лидера в системе. Или репликацию между несколькими серверами. Так как со со "стороны записей" нет понимания о репликации. Нам хочется описывать сущности с которыми работает система, когда надо проверить порядок на них, или увидеть как они взаимодействуют между собой.

Примеры:
* Acceptor-ы и Proposer-ы в Paxos
* Leader и follwer-ы в Raft и Kafka

 и обмениваются сообщениями

Примеры:
* Запрос на участие в выборах лидера в Kafka-е
* Запросы Promise и Accept в Paxos
* Запросы во время выборов лидера и репликацию в Raft-е

### Моделирование мира
Займемся моделированием мира, в котором функционирует наша система: опишем, как функционирует сеть, с какой скоростью работают узлы, как течет время и т.п.

Есть 2 модели: асинхронная и синхронная. В первой нет ограничений на
* время доставки сообщений
* дрейф часов
* относительную скорость работы узлов.

В синхронной модели эти ограничения есть.

Реальность где-то посередине. Например: реальная сеть большую часть времени ведет себя синхронно, но никакая сетевая инфраструктура не может гарантировать это на 100% времени. Бывают периоды нестабильности, когда сеть задерживает и теряет сообщения.

Мы будем использовать асинхронную модель. Мы сильно пессимизируем, когда описываем систему в этой модели, так как в реальности сеть не бывает бесконечно долгой. Это можно делать, так как выполнения в реальном мире - это подмножество в выполнениях в асинхронной модели, поэтому, если мы убедимся в корректности алгоритма/системы в модели без ограничений, то и с ними он будет работать. Так же описать асинхронную модель проще.

Распределенные системы скрывают отказы внутри себя и остается доступны после падения нескольких узлов и мы хотим в этом убедиться.

Поговорим подробнее про каждую из состовляющих моделию.

### Моделирование сети и отправки сообщений
Сеть, через которую взаимодействуют узлы распределенной системы, – это провода и промежуточные устройства: коммутаторы и роутеры. Сообщение, отправленное в сеть, существует либо в виде сигнала внутри
провода, либо в виде байт в буфере промежуточного устройства.

В TLA мы можем абстрагироваться от физических подробностей и представить сеть просто как множество in-flight сообщений.

Пример

    VARIABLE msgs

Отправка собщения - это добавление его в это множество.

Пример:

В Paxos:

    Send(m) == msgs' = msgs \cup {m}

Отправка сообщения proposer-ом:

    Phase1a(b) == /\ Send([type |-> "1a", bal |-> b])
                  /\ UNCHANGED <<maxBal, maxVBal, maxVal>>

Получение собщение acceptor-ом и ответ на него:

    Phase1b(a) == /\ \E m \in msgs :
                      /\ m.type = "1a"
                      /\ m.bal > maxBal[a]
                      /\ maxBal' = [maxBal EXCEPT ![a] = m.bal]
                      /\ Send([type |-> "1b", acc |-> a, bal |-> m.bal,
                                mbal |-> maxVBal[a], mval |-> maxVal[a]])
                  /\ UNCHANGED <<maxVBal, maxVal>>

В Kafka:

    ControllerUpdateIsr(newLeader, newIsr) == \E newLeaderEpoch \in LeaderEpochSeq!IdSet :
        /\ LeaderEpochSeq!NextId(newLeaderEpoch)
        /\  LET newControllerState == [
                leader |-> newLeader,
                leaderEpoch |-> newLeaderEpoch,
                isr |-> newIsr]
            IN  /\ quorumState' = newControllerState
                /\ leaderAndIsrRequests' = leaderAndIsrRequests \union {newControllerState}


В асинхронной сети существует насколько проблем, которые учитывают при проектировании системы:
* Разделение
* Задержка сообщшений
* Потеря сообщений
* Дублирование сообщений

Для моделирования всех этих особенностей используется конструкция \E val \in set: P(val), которая выдает произвольный элемент множества, удовлетворяющий предикату. Таким образом, у нас нет никакого порядка на элементах множества. Это помогает моделировать reordering на отправленных сообщениях, задержки соощений и их потерю, так как элемент может никогда не взяться из множества.

Разделение сети тоже получается промоделировать такой конструкцией, так как model checker исследует траектории, где узлы будут получать сообщения только от какого-то подмножества всех участников.

Например:

В Paxos проверяется, что у сообщениия, которое получено acceptor-ом на фазе 1b и 2b ballot больше, чем локальный у acceptor-а

    Phase1b(a) == /\ \E m \in msgs:
                      /\ m.type = "1a"
                      /\ m.bal > maxBal[a]
                      ...

   Phase2b(a) == \E m \in msgs : /\ m.type = "2a"
                                 /\ m.bal \geq maxBal[a]
                                 ...

В Raft-е при получении сообщения проверяется в каком term-е оно было отправлено, чтобы не обрабатывать сообщения из прошлого.

    HandleRequestVoteResponse(i, j, m) ==
        \* This tallies votes even when the current state is not Candidate, but
        \* they won't be looked at, so it doesn't matter.
        /\ m.mterm = currentTerm[i]
        ...

Удалять сообщения из этого множества ненужно, так как в реальном мире сообщения могут дублироваться или никогда не доставляться. Конструкция с \E полностью удовлетворяет этому поведению, так как некоторые сообщения могут вообще никогда не обработаться, а некоторые обработаться несколько раз. Если же удалять сообщения после обработки из сета, то мы отказываемся от дедубликации и потери сообщений. Нам надо самим добавить экшены, которые будут моделировать такие ситуации.

Так сделали в Raft

    DuplicateMessage(m) ==
        /\ Send(m)
    /\ UNCHANGED <<serverVars, candidateVars, leaderVars, logVars>>

    DropMessage(m) ==
        /\ Discard(m)
    /\ UNCHANGED <<serverVars, candidateVars, leaderVars, logVars>>

Большинство распределенных алгоритмов разбито на фазы: в Raft - выбор лидера и переход между фазами репликации происходит при достижении кворума, в Paxos - выборы значения происходит так же при достижении кворума. Поэтому моделировать point-2-point общение не нужно. Кворумы используются для маскировки сбоев: операция успешно завершается, даже если не все узлы системы отвечают. Так что почти всегда узлы в распределенном алгоритме отправляют сообщение не отдельным учестникам, а всем узлам и дожидаются подтверждения/ответа от произвольного кворума.

Чтобы моделировать отправку сообщений всем участникам система и сэкономить на состояниях графа конфигураций, можно, например, не указывать явных адресатов.

Например в Paxos, сообщения от proposer-ов отправляются всем acceptor-ам сразу. В них не указывается адресат:

    /\ Send([type |-> "1a", bal |-> b])

Таким образом, узле отправляет сообщение всем остальным участникам.

Но важно, что в сообщениях, которые приходят от acceptor-а к proposer-у надо указывать адресата и отправителя. Информацию о адресате передает bal, который отправляется в сообщение. А отправитель указывается явно, чтобы proposer понимал, когда он соберет нужное кол-во ответов от acceptor-ов.

    /\ Send([type |-> "1b", acc |-> a, bal |-> m.bal,
             mbal |-> maxVBal[a], mval |-> maxVal[a]])

Такие техники упрощают алгоритм по сравнению с реальным миром и уменьшают гранулярность событий, так что существует вероятность потерять какие-то сложные сценарии с гонками и надо быть аккурантым.

### Сбои
Существуют 3 основные модели сбоев:
* Отказ - узел взрывается и больше никогда не принимает участия в системе.
* Рестарт - узел не отвечал некоторое время, а потом опять начал участвовать в системе.
* Византийский - узел можно отходить от протокола общения.

С моделирование отказов все просто. Так как мы выбрали асинхронную систему, у нас нет возможности отличить умерший узел и тот, который долго обрабатывает наш запрос.

Авторы FLP пишут в [статье](https://groups.csail.mit.edu/tds/papers/Lynch/jacm85.pdf), что они не различают умерший узел от той, которая долго не отвечает. В рамках графа конфигураций в пути просто не будет состояний, связанных с умершим узлом. В TLA+ аналогично model checker исследует такие состояния, где узел модет не отвечать.

В TLA+ можно промоделировать рестарт узла отдельным экшеном, который сбрасывает все волатильное состояние. Поддержка рестртов требует от алгоритма хранить часть своего состояния в персистентом хранилища.

Например, в RAFT реплика должна отдавать не более одного голоса в пределах одного терма, если после рестарта реплика забудет, что она уже отдала свой голос за кого-то, то в этом же терме она пожет проголосовать еще раз. Тогда несколько узлов будут считать, что они лидеры. Для того чтобы избежать этого, перед ответом на RequestVote узел должен прочитать, за кого он отдал в текущем терме свой голос.

В Basic Paxos аксептор после Promise(b) на первой фазе должен игнорировать / отклонять сообщения с меньшими номерами.
1) Для этого аксептор запоминает максимальный ballot number, который он получал от proposer-ов.
2) Запоминать этот номер нужно обязательно в персистентом хранилище перед отправкой Promise / Accept, чтобы после рестарта не забыть о данном обещании.

В спеке Basic Paxos рестарты не моделируются, а в Raft-е есть отдельный экшен

    \* Server i restarts from stable storage.
    \* It loses everything but its currentTerm, votedFor, and log.
    Restart(i) ==
        /\ state'          = [state EXCEPT ![i] = Follower]
        /\ votesResponded' = [votesResponded EXCEPT ![i] = {}]
        /\ votesGranted'
        /\ voterLog'
        /\ nextIndex'
        /\ matchIndex'
        /\ commitIndex'
        /\ UNCHANGED <<messages, currentTerm, votedFor, log, elections>>


Самое интересное тут UNCHANGED переменные, которые во время экшена не изменяются. Среди них есть votedFor - это кандидат за которого сервер проголосовал в текущем раунде. Именно эти переменные находятся не локальну у узла, а в персистентом хранилище, чтобы при рестарте узла не потерять их значения. Еще нужно заметить, что в unchanged помимо состояния реплики есть еще и “глобальные” переменные, которые отвечают за состояния всей сети (messages) и переменные, которые используют для докозательства (elections)

Византийский отказы - ...

### Недетерминизм
Недетерминизм является одним из главных источников сложности проектирования распределенных систем. Недетерминизм возникает естественным образом из-за реактивной природы распределенных систем: они реагируют на внешние воздействия (запросы клиентов) и подвержены влиянию неконтролируемой среды (сеть, течение времени), железа / runtime-а  (паузы gc) и сбоев.

Примером недетерминизма в многопоточных программах может служить поведение планировщика ОС. Заранее неизвестно, когда планировщик будет производить переключение потоков и порядок на них. Из-за этого тестирование становится очень сложным. В распределенных системах недетерминизм заключается во времени доставки сообщений, их потери или дубликации, в сбоях и рестартах узлов системы, в запросах клиентов, которые будут поступать. Так же выборы лидера, которые используется в многих алгоритмах, играют важную роль в недетерминизме, так как заранее нельзя сказать, какая последовательность узлов будет выбрана.

Какие есть способы выразить недетерминизм в TLA?
* Одним из способов описать недетерминизм системы - это использование дизъюнкции в экшенах. Например, у нас клиент может отправить SELECT или INSERT на узел БД, тогда мы можем в Next добавить дизъюнкцию этих двух событий (Next == Select \/ Insert). Никакой гарантии на порядок взятия экшенов нет, а значит model checker должен проверить все возможные порядки на этих дейсвтиях.
* Еще один способ моделировать недетерминизм - это оператор \E. он используется для того, чтобы выбрать, какой из узлов системы следующим сделает “ход” - получит сообщение, перезагрузится и т.п.

Примеры:

В спеке Raft-а выбирается какое сообщение будет доставлено или какой из узлов выполнит действие

    Next == /\ \/ \E i \in Server : Restart(i)
               \/ \E i \in Server : Timeout(i)
               \/ \E i,j \in Server : RequestVote(i, j)
               \/ \E i \in Server : BecomeLeader(i)
               \/ \E i \in Server, v \in Value : ClientRequest(i, v)
               \/ \E i \in Server : AdvanceCommitIndex(i)
               \/ \E i,j \in Server : AppendEntries(i, j)
               \/ \E m \in DOMAIN messages : Receive(m)
               \/ \E m \in DOMAIN messages : DuplicateMessage(m)
               \/ \E m \in DOMAIN messages : DropMessage(m)


В спеке Paxos-а выбирается какое сообщение будет обработано или какой из acceptor-ов ответит на запрос

    Phase2b(a) == \E m \in msgs : /\ m.type = "2a"
                                  /\ m.bal \geq maxBal[a]
                                  /\ maxBal' = [maxBal EXCEPT ![a] = m.bal]
                                  /\ maxVBal' = [maxVBal EXCEPT ![a] = m.bal]
                                  /\ maxVal' = [maxVal EXCEPT ![a] = m.val]
                                  /\ Send([type |-> "2b", acc |-> a,
                                           bal |-> m.bal, val |-> m.val])

    Next == \/ \E b \in Ballot : \/ Phase1a(b)
                                \/ \E v \in Value : Phase2a(b, v)
           \/ \E a \in Acceptor : Phase1b(a) \/ Phase2b(a)

В Kafka-е:

    ControllerShrinkIsr == \E replica \in Replicas :
        /\  \/  /\ quorumState.leader = replica
                /\ quorumState.isr = {replica}
                /\ ControllerUpdateIsr(None, quorumState.isr)
            \/  /\ quorumState.leader = replica
                /\ quorumState.isr # {replica}
                /\ ControllerUpdateIsr(None, quorumState.isr \ {replica})
            \/  /\ quorumState.leader # replica
                /\ replica \in quorumState.isr
                /\ quorumState.isr # {replica}
                /\ ControllerUpdateIsr(quorumState.leader, quorumState.isr \ {replica})
        /\ UNCHANGED <<nextRecordId, replicaLog, replicaState>>

В одном экшене все действия происходят за один раз

Например:

В экшене для acceptor-а в первой фазе алгоритма Паксоса

    Phase1b(a) == /\ \E m \in msgs :
                      /\ m.type = "1a"
                      /\ m.bal > maxBal[a]
                      /\ maxBal' = [maxBal EXCEPT ![a] = m.bal]
                      /\ Send([type |-> "1b", acc |-> a, bal |-> m.bal,
                                mbal |-> maxVBal[a], mval |-> maxVal[a]])
                  /\ UNCHANGED <<maxVBal, maxVal>>

acceptor получает сообщение от proposer-а. Проверяет, что ballot у сообщения больше, чем текущий локальный у acceptor-а, и если это так, то он обновляет максимально полученный ballot и отвечает текущему proposer-у.

В этом экшене инкапсулированны 3 дейтсвия, которые произошли в системе.

Можно провести аналогия с FLP. В док-ве авторы в переходе между состояниями объединяют 3 действия: получение сообщения, изменение состояния и отправку сообщения. Так как граф конфигураций в FLP и в TLA+ однотипны, то можно поступить аналогично и в нашей спеке

В экшене TLA происходит все аналогично. Надо очень аккуратно выбрать действия, которые мы будем делать атомарно в спеке. Для распределенных систем это сделать проще, так как почти все внутренние действия узлов для изменения состояния рассматриваются до отправки нового сообщения, и для других участников системы это происходит как будто атомарно.

Для параллельных алгоритмов все сложнее, потому что, если использовать в PlusCal-е однин лэйбл (это те действия, которые происходят атомарно) можно потерять гонки, которые происходят в алгоритме, так как мы можем написать в лэбле чтение из одной ячейки и запись в другую.

### Cвойства системы
Требование к системе/алгоритму мы формулируем в виде свойств. Свойство - это набор траекторий в графе состояний.

Будем рассматривать 2 вида свойста:
* Safety свойство говорит о том, что если свойство нарушено на траектории, то существует перфикс на котором это свойство нарушается и на любом расширении префикса свойство будет нарушено. Эти свойство должны выполняться в каждом состоянии нашей системы.
* Liveness свойство говорит о том, что для любого бесконечного поведение существует состояние, в котором свойство выполняется.

При верификации распределенных алгоритмов достаточно проверять только свойства двух приведенных типов: любое свойство можно выразить как пересечение свойств safety и liveness (https://pron.github.io/posts/tlaplus_part3#safety-and-liveness)

Приведем примеры safety свойств:

Пример инварианта в RAFT: “В каждом терме может быть не более одного лидера”. В TLA+ это выражено отрицанием safety свойства MoreThanOneLeader:

    BothLeader( i, j ) ==
         /\ i /= j
         /\ currentTerm[i] = currentTerm[j]
         /\ state[i] = Leader
         /\ state[j] = Leader

    MoreThanOneLeader ==
        \E i, j \in Server :  BothLeader( i, j )

В Paxos главным свойством является то, что если в какой-то из фаз было выбрано значение v с proposal number b, то для всех сообщений propose(v', p'), где p' > p => v' = v. В TLA+ оно выражено safety свойством "NoFutureProposal":

    Agreed(v,b) == \E Q \in Quorum2: \A a \in Q: Sent2b(a,v,b)
    NoFutureProposal(v,b) == \A v2 \in Value: \A b2 \in Ballot: (b2 > b /\ Sent2a(v2,b2)) => v=v2

    SafeValue == \A v \in Value: \A b \in Ballot: Agreed(v,b) => NoFutureProposal(v,b)

В Kafka аппенды, которые подтверждены лидером, должны находиться в логах всех реплик кворума. В спеке TLA+ это свойство StrongIsr:

    StrongIsr == \A r1 \in Replicas :
        \/ ~ ReplicaPresumesLeadership(r1)
        \/ LET  hw == replicaState[r1].hw
           IN   \/ hw = 0
                \/ \A r2 \in quorumState.isr, offset \in 0 .. (hw - 1) : \E record \in LogRecords :
                    /\ ReplicaLog!HasEntry(r1, record, offset)
                    /\ ReplicaLog!HasEntry(r2, record, offset)

Свойство WeakIsr отличается тем, в нем проверяется, что запись присутсвует у всех реплик, которые текущая считает своим ISR.

Termination является одним из liveness свойств, но для распределенных систем мы не можем говорить о прекращении работы из-за свойства реактивности.

Liveness свойства пости никогда не применяются для распределенных систем, так как о завершении можно говорить, только в случае частично синхронной или синхронной сети, а в TLA+ мы не моделируем их.

### Проврека моделей согласованности
Если мы смотрим на систему не изнутри, а снаружи, то все алгоритмы от нас скрыты, нам доступно лишь наблюдаемое поведение, кооторое формулизуется в виде моделей согласованности.

В болшинстве систем доступны 2 модели согласованности: eventual consistency и linearizability. Мы рассматриваем именно linearizability, так как оно наиболее понятно пользователю описывает поведение системы и является одной из самых строгих гарантий.

Для описания таких свойста надо иметь историю в каком-нибудь виде, так как они основываются на порядке выполнения операций.

В TLA+ для этого логично использовать tuple, куда надо сохранять события, которые произошли в системе (например действия транзакций, начало и конец вставок в DataStore и т.д.). С помощью этой служебной переменной мы сможем проверять интересующие нас свойства. Для работы с историей будет удобно написать операторы, чтобы работать с определенным типом событий.

    ActiveOrFinalizedTxns(h) == {e.txnid : e \in Range(h)}        (* all transactions apart from those that have not yet started *)
    NotYetStartedTxns(h)     == TxnId \ ActiveOrFinalizedTxns(h)
    CommittedTxns(h)         == {e.txnid : e \in SelectEvents(h, LAMBDA e : e.op \in {"commit"})}
    AbortedTxns(h)           == {e.txnid : e \in SelectEvents(h, LAMBDA e : e.op \in {"abort"})}
    FinalizedTxns(h)         == CommittedTxns(h) \union AbortedTxns(h)
    ActiveTxns(h)            == ActiveOrFinalizedTxns(h) \ FinalizedTxns(h)

Например: алгоритм SI гарантирует Serializability. Для этого мы должны проверить, что конкурирующие транзакции можно упорядочить (они не образуют цикл) и не нарушаются инварианты алгоритма на запись и чтение. Например, что если две транзакции конкурируют, а их write set-ы пересекаются, то применится только одна из них у которой временная метка меньше, а другая будет отменена.

    BernsteinSerializable(h) ==  ~ IsCycle(BernsteinMVSG(h))

    SemanticsOfSnapshotIsolation ==
        /\ CorrectReadView
        /\ FirstCommitterWins

Заметим, что можно явно не моделировать историю событий. Например в Paxos историю можно восстановить из множества сообщений, которые находятся в системе, используя Ballot и фазу у сообщений.


### Обрезки

Когда мы пишем распределенную систему нам удобно рассуждать с точки зрения наблюдателя. Моделировать компоненты, которые играют важную роль в нашей системе (узлы, клиент и e.t.c). Описать систему, как ее видит польщователь, так ка это даст наибольшую интуитивность.

Например:

В Paxos есть отдельные сущности для acceptor-ов, системы кворумов, которые играют важную роль в алгоритме.

    CONSTANT Value, Acceptor, Quorum1, Quorum2

Действия proposer-ов моделируют отдельными экшенами, которые предлагают значения.

В Raft явно моделируются узлы, которые участвуют в алгоритме:

    \* The set of server IDs
    CONSTANTS Server

Аналогично происходит в Kafka-е


Можно моделировать не участников системы, а описать поведение объектов, с которыми эта система работает (описать как будут себя вести компоненты сервиса). Пусть у нас есть лог с записями. Можем моделировать не клиента, который эти записи делает, а сами записи изнутри, то есть описать как записи взаимодействуют друг с другом.

Такой подход используется в спеке SI. Там моделируется конечное число транзакций, которые поступают в систему.

И для них описываются их основные действия. Начало, конец, аборт и т.п.

    BeginEventsT   == [op : {"begin"},  txnid : TxnId]
    AbortEventsT   == [op : {"abort"},  txnid : TxnId, reason : AbortReasons]
    CommitEventsT  == [op : {"commit"}, txnid : TxnId]
    ReadEventsT    == [op : {"read"},   txnid : TxnId, key : Key, ver : TxnId]
    WriteEventsT   == [op : {"write"},  txnid : TxnId, key : Key]

Вот, что Лампорт пишет об этом:
"Mathematical manipulation of specifications can yield new insight. A producer/consumer system can be written as the conjunction of two formulas, representing the producer and consumer processes. Simple mathematics allows us to rewrite the same specification as the conjunction of n formulas, each representing a single buffer element. We can view the system not only as the composition of a producer and a consumer process, but also as the composition of n buffer-element processes. Processes are not fundamental components of a system, but abstractions that we impose on it. This insight could not have come from writing specifications in a language whose basic component is the process."

Это все сильно завязано на свойства, которые мы хотим проверять. Нам не интересно моделировать записи в логе, когда мы хотим проверить, например, алгоритм для выбора лидера в системе. Или репликацию между несколькими серверами. Так как со со "стороны записей" нет понимания о репликации. Нам хочется описывать сущности с которыми работает система, когда надо проверить порядок на них, или увидеть как они взаимодействуют между собой.

Примеры:
* Acceptor-ы и Proposer-ы в Paxos
* Leader и follwer-ы в Raft и Kafka

 и обмениваются сообщениями

Примеры:
* Запрос на участие в выборах лидера в Kafka-е
* Запросы Promise и Accept в Paxos
* Запросы во время выборов лидера и репликацию в Raft-е

-------------------

Каждая траектория в спеке - это бесконечный набор состояний системы, но это не значит, что она не пригодна для model checker-а. В основе TLC лежит обход в ширину графа состояний, из-за этого чекеру не страшно, что траектории бесконечные, так как они из себя представляют циклы разной длины, потому что множество состояний системы конечно.

------------------

В большинстве спек темпоральность TLA+ проявляется в виде `[]` перед экшеном Next
