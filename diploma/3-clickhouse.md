## Общие слова о системе
ClickHouse - столбцовая система управления базами данных (СУБД) для онлайн обработки аналитических запросов (OLAP), которая разрабатывается компанией Яндекс и open-source сообществом.

В этой главе мы займемся спецификацией и верификацией деталей протокола репликации этой системы.

### Распределенность
Модель данных КХ оперирует таблицами. Каждая таблица реализуется определенным движком, который отвечает за механизм хранения данных и процесс обработки клиентских запросов.

КХ поддерживает горизонтальное масштабирование с помощью распределенных шардированных таблиц, которые реализуются движком "Distributed"

КХ - отказоустойчивая система, где каждый шард распределенной таблицы независимо реплицируется, протокол репликации инкапсулирован в семействе движков "Replicated"

### Репликация
* Вставки в таблицу-шард выполняются блоками
* Вставки выполняются через разные реплики шарда, реплики узнают о вставке на другие узлы через ZooKeeper и скачивают соответсвующий блок напрямую у других реплик, которые в текущий момент имеют связь с ZK
* Для обнаружения "мертвых" реплик, в ZooKeeper хранится эфимерная нода is_active, которая показывет, есть ли у узла связь с ZooKeeper-ом. Будем называть такие реплики - активными.
* Блоки с данными хранятся в отдельных файлах, для оптимизации чтения эти файлы нужно объединять между собой.
* Для того, чтобы реплики сходились к одному состоянию, они должны договориться о порядке вставок и слияний, для этого они испольщую лог в  ZooKeeper
* Информация о вставке попадает в лог после того, как реплика обработала запрос пользователя и записала данные к себе на диск.
* Для назначения слияний испольщуется лидер, который выбирается с помощью ZooKeeper. Он назначает слияния и вставляет информацию об этом в лог

### Отчистка лога
Бесконечно хранить весь лог апдейтов в ZooKeeper невозможно, нужно лишь поддерживать акутальный хвост, а старые записи, которые все реплики уже обработали, можно удалить.

Мы можем удалить старые команды, которые обработали все реплики, так как существующие реплики их уже никогда не увидят, поскольку не двигают свой итератор только вперед, а новые при старте будут копировать состояние с активных реплик.

Во время очистки будем удалять из лога записи до минимального log pointer-а активных реплик.

Реплика может восстановить связь с ZK и снова стать активной. В этот момент ей надо понять, удалили ли нужные ей записи из лога. Для этого во время очистки лога будем помечать неактивные реплики, для которых мы удалили нужные записи, флагом is_lost в ZooKeeper.

Когда такая реплика снова станет активной, она может понять, надо ли ей копировать состояние с другой реплики или можно просто продолжать выполнять лог.

### Quorum Insert/Select
В обычном режиме реплика подтверждает вставку клиенту только после того, как она запишет себе на диск.

Иногда клиенту хочется обеспечить большую надежность и получить подтверждение о вставке, когда его данные записились на какое-то число реплик. Для этого в ClickHouse используется режим кворумных вставок.

Реплика, на которую прилетела кворумная вставка, создает znode в ZooKeeper, куда записывает себя и после добавляет информацию про это в лог. Когда другие реплики дойдут до этой записи в логе, они посмотрят набрался ли кворум для этой вставки, если нет, то обновят znode в ZooKeeper, добавя себя в тех, кто записал этот кусок данных.

Когда в ноде для кворума соберется нужное количетсво реплик, то нода удалится и запись подтвердится клиенту.

КХ поддерживает 2 модели согласованности:
* eventual consistency
* "inearizability"

Для второй во время ферификации были найдены ошибки.

Для Линеаризуемости КХ использует чтения и вставка с кворумом.

Вставка: Клиент может выбрать сколько реплик должно скопировать себе вставленный кусок, чтобы ему подтвердили вставку. Для этого процесса в ZooKeeper есть специальная znode, в которой записана информация о текущем кворуме: кол-во реплик, которые уже скопировали себе кусок, номер последнего вставленного куска.

Когда кол-во реплик нараблось, то znode с информацией о текущем кворуме удаляется из ZooKeeper, а znode с информацией о последнем вставленном значении обновляется. Если в момент репликации кусок становится невозможно скачать, например, когда реплика, которая вставила стала неактивной, то он помещается в znode "failed_parts".

Во время чтения можно включить также настройку sequential_consistency, чтобы читать данные с реплики со св-вом линеаризуесомти. Для этого в момент чтения реплика, на которую прилетел запрос идет в znode и забирает номер последнего втсавленного куска. И если у нее есть все блоки меньшего номера, которые не лежат в failed  parts, то она отвечает на запрос пользователя, если их нет, то она отвечает, что пока не может обработать этот запрос.

## Уровень абстракции и участники
Определимся с уровнем абстракции

Система очень похожа на Kafka. Так как почти вся синхронизация проиходит внутри ZooKeeper-а. И реплики между собой почти не взаимодействуют, они только пересылает куски с данными между собой. Моделировать эти общения не надо, в отличие от Kafka, так как в ставке принимает участие ZooKeeper. Будем полагать, что система получила кусок данных, когда прочитала информацию об ставке их в ZooKeeper.

Сситаем, что znode - это распределенная отказоустойчивая ячейка памяти.

    VARIABLES
        \* Log in zookeeper.
        log,

        \* State replica. Active or not and e.t.c
        replicaState

## Участники системы
Основными действующими лицами у нас будут реплики и клиент. Для лучшей читаемости кода объединим действия для каждой из сущностей.

    ReplicaAction ==
        \E replica \in Replicas:
            \/ ExecuteInsert(replica)
            \/ ExecuteMerge(replica)
            \/ BecomeLeader(replica)

    ClientAction ==
        \/ Insert
        \/ Select

## Сбои
Так же как и другие рассмотренные алгоритма, ClickHouse работает в асинхронной модели, где узлы могут отказывать и рестартовать.

В спеке явно промоделированы рестарты:

    ReplicaBecomeInactive ==
        /\ \E replica \in Replicas :
          /\ IsActive(replica)
          /\ RepicaStartInactive(replica)
          ...

    ReplicaBecomeActive ==
        /\ \E replica \in Replicas :
          /\ ~IsActive(replica)
          /\ RepicaStartActive(replica)
          ...

Вся информация о реплике хранится в спецальной znode. Так что после рестарта реплика восстанавливает состояние, которое у нее было поледним.

## Недетерминизм
Недетерминизм в системе проявляется в нескольких моментах:

Клиент может послать свой запрос на любую из реплик. Это моделируется в экшенах для клиента:

    QuorumReadLin ==
        ...
        /\ \E replica \in Replicas :
            /\ IsActive(replica)
            /\ Cardinality(GetCommitedId) = Cardinality(replicaState[replica].local_parts \ ({quorum.data} \cup GetData(failedParts)))
        ...

 Так же, чтобы описать, что любая реплика может совершить действие в произвольный момент, воспользуемся аналогичной конструкцией в экшене для реплик.

     ReplicaAction ==
         \E replica \in Replicas:
          \/ IsActive(replica)
              /\ \/ ExecuteLog(replica)
                 \/ UpdateQuorum(replica)
                 \/ EndQuorum(replica)
                 \/ BecomeInactive(replica)
                 \/ FailedQuorum(replica)
          \/ ~IsActive(replica)
              /\ BecomeActive(replica)

## Конечная спека
Для того, чтобы протестировать систему надо сделать конечным число действий в системе и кол-во участников.

Мы ограничим возможное кол-во реплик, которые будут в системе, так же нало ограничить кол-во запросов от польщователя. Для этого воспользуемся приемом из спеки SI и объявисм множество уникальных id для вставок.

    CONSTANTS
        Replicas,
        RecordsId,
        ...

Чтобы TLC не обнаруживал дедлок заведем экшен, в котором будем проверять, что id у всех записей в логе совпадают с этим множеством.

    LegitimateTermination ==
        /\ GetIds(log) = RecordsId
        ...

В спеке Кворум клиент можеть выполнять не только вставки, но и selecet из нашей БД. Поэтому для них мы тоже заведем аналогичный сет. И проверку для его:

LegitimateTermination ==
    /\ GetIds(failedParts) \cup GetCommitedId = RecordsId
    /\ Cardinality(GetSelectFromHistory(history)) = HistoryLength
    ...

### Тестирование

Unit-тестирование будет уместно испольщовать в спеке про кворум, так как там есть операторы, которые вычленяют из истории нужные нам действия.

Приведем пример истории и убедимся, что операторы работают правильно:

    H == <<[type |-> "StartInsert", record_id |-> 1],
     [type |-> "Read", record_id |-> 2],
     [type |-> "FailedInsert", record_id |-> 3],
     [type |-> "EndInsert", record_id |-> 4],
     [type |-> "EndInsert", record_id |-> 5],
     [type |-> "Read", record_id |-> 6]>>

    Unit_test == GetSelectFromHistory(H) = {[type |-> "Read", record_id |-> 2], [type |-> "Read", record_id |-> 6]}

Проверим, что инвариант в спеке про обрезку лога

    ValidLogPointer == [] (\A replica \in Replicas: IsActive(replica) => deletedPrefix < replicaState[replica].log_pointer)

обнаруживает, когда удалили записи из лога у активной реплики.

Для этого воспользуемся мутационным тестированием и изменим на

    ClearOldLog ==
        /\ Len(log) > 0
        /\ deletedPrefix' = Max({Min(GetLogPointers), deletedPrefix})
        ...

        ClearOldLog ==
            /\ Len(log) > 0
            /\ deletedPrefix' = Max({Max(GetLogPointers), deletedPrefix})
            ...

То есть удаляем не по минимальную запись у активных реплик, а по максимальную.

Запустив TLC обнаружим, что инвариант не выполняется.

Отдельного внимания заслуживает спека про Кворумы.

Одниой из главных проверок служит то, что история будет линеаризуема.

Для этого мы по определению проверим все действия в истории.

Чтение отдает результат, только если у реплики есть все куски, который были втсавленны с кворуом

    QuorumReadLin ==
        /\ Len(log) > 0
        /\ Cardinality(GetSelectFromHistory(history)) < HistoryLength
        /\ \E replica \in Replicas :
            /\ IsActive(replica)
            /\ Cardinality(GetCommitedId) = Cardinality(replicaState[replica].local_parts \ ({quorum.data} \cup GetData(failedParts)))
            /\ LET max_data == Max(replicaState[replica].local_parts \ ({quorum.data} \cup GetData(failedParts)))
               IN /\ max_data # NONE
                  /\ ReadEvent(GetIdForData(max_data, log))
        /\ UNCHANGED <<log, replicaState, nextRecordData, quorum, lastAddeddata, failedParts>>


Но в КХ есть проблема, что во время чтения znode про максимальный блок, который мы вставили не происходит sync у узлов ZooKeeper-а, таким образом, мы могли получить устаревшее значение этого параметра. Если реплика, на которую мы отправили запрос, пошла на узел в ZooKeeper, который не подтянул все изменения.

Значит, мы можем получить вообще любое значение параметра про послелдний добавленный кусок.

Это мы промоделируем экшеном

    QuorumReadWithoutLin ==
        /\ Len(log) > 0
        /\ Cardinality(GetSelectFromHistory(history)) < HistoryLength
        /\ \E replica \in Replicas :
            /\ IsActive(replica)
            /\ LET max_data == Max(replicaState[replica].local_parts \ ({quorum.data} \cup GetData(failedParts)))
               IN /\ max_data # NONE
                  /\ ReadEvent(GetIdForData(max_data, log))
        /\ UNCHANGED <<log, replicaState, nextRecordData, quorum, lastAddeddata, failedParts>>

Если запустить проверку линеаризуемости с ним, то TLC выдаст о нарушении инварианта.
