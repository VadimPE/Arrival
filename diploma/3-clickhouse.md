## Общие слова о системе
ClickHouse -- столбцовая система управления базами данных (СУБД) для онлайн обработки аналитических запросов (OLAP), которая разрабатывается компанией Яндекс и open-source сообществом.

В этой главе мы займемся спецификацией и верификацией деталей протокола репликации этой системы.

### Распределенность
Модель данных КХ оперирует таблицами. Каждая таблица реализуется определенным движком, который отвечает за механизм хранения данных и процесс обработки клиентских запросов.

КХ поддерживает горизонтальное масштабирование с помощью распределенных шардированных таблиц, которые реализуются движком "Distributed".

КХ -- отказоустойчивая система, где каждый шард распределенной таблицы независимо реплицируется, протокол репликации инкапсулирован в семействе движков "Replicated".

### Репликация
* Вставки в таблицу-шард выполняются блоками
* Вставки выполняются через разные реплики шарда, реплики узнают о вставке на другие узлы через ZooKeeper и скачивают соответсвующий блок напрямую у других реплик, которые в текущий момент имеют связь с ZK
* Для обнаружения "мертвых" реплик, в ZooKeeper хранится эфимерная нода is_active, которая показывет, есть ли у узла связь с ZooKeeper-ом. Будем называть такие реплики - активными.
* Блоки с данными хранятся в отдельных файлах, для оптимизации чтения эти файлы нужно объединять между собой.
* Для того, чтобы реплики сходились к одному состоянию, они должны договориться о порядке вставок и слияний, для этого они испольщую лог в  ZooKeeper
* Информация о вставке попадает в лог после того, как реплика обработала запрос пользователя и записала данные к себе на диск.
* Для назначения слияний испольщуется лидер, который выбирается с помощью ZooKeeper. Он назначает слияния и вставляет информацию об этом в лог

### Отчистка лога
Бесконечно хранить весь лог апдейтов в ZooKeeper невозможно, нужно лишь поддерживать акутальный хвост, а старые записи, которые все реплики уже обработали, можно удалить.

Мы можем удалить старые команды, которые обработали все реплики, так как существующие реплики их уже никогда не увидят, поскольку не двигают свой итератор только вперед, а новые при старте будут копировать состояние с активных реплик.

Во время очистки будем удалять из лога записи до минимального log pointer-а активных реплик.

Реплика может восстановить связь с ZK и снова стать активной. В этот момент ей надо понять, удалили ли нужные ей записи из лога. Для этого во время очистки лога будем помечать неактивные реплики, для которых мы удалили нужные записи, флагом is_lost в ZooKeeper.

Когда такая реплика снова станет активной, она может понять, надо ли ей копировать состояние с другой реплики или можно просто продолжать выполнять лог.

### Quorum Insert/Select
В обычном режиме реплика подтверждает вставку клиенту только после того, как она запишет себе на диск.

Иногда клиенту хочется обеспечить большую надежность и получить подтверждение о вставке, когда его данные записились на какое-то число реплик. Для этого в ClickHouse используется режим кворумных вставок.

Реплика, на которую прилетела кворумная вставка, создает znode в ZooKeeper, куда записывает себя и после добавляет информацию про это в лог. Когда другие реплики дойдут до этой записи в логе, они посмотрят набрался ли кворум для этой вставки, если нет, то обновят znode в ZooKeeper, добавя себя в тех, кто записал этот кусок данных.

Когда в ноде для кворума соберется нужное количетсво реплик, то нода удалится и запись подтвердится клиенту, если по какой-то причине реплика, которая хочет принять участие в кворуме не может получить этот блок, то кворум считается провалившимся и информация об этом записывается в ZooKeeper.

Так же, кроме вставок, КХ поддерживает чтения с кворумом, для этого в ZooKeeper хранится порядковый номер последнего вставленного блока.

Когда на реплику приходит Select, то реплика отвечает на него, только если у нее есть все блоки, которые быди вставлены с кворумом, кроме тех, для которых провалился кворум.

znode с номером последнего вставленного блока обновляется, когда набирапется кворум на вставку.

С помощью такого механизма удается добиться линеаризуемости вставок и чтений.

## Моделирование
Определимся с уровнем абстракции

Система очень похожа на Kafka. Так как почти вся синхронизация проиходит внутри ZooKeeper-а. И реплики между собой почти не взаимодействуют, а только пересылает куски с данными между собой. Моделировать эти общения не надо, в отличие от Kafka, так как в ставке принимает участие ZooKeeper. Будем полагать, что система получила кусок данных, когда прочитала информацию об ставке их в ZooKeeper.

Основными действующими лицами у нас будут реплики и клиент. Для лучшей читаемости кода объединим действия для каждой из сущностей.

    ReplicaAction ==
        \E replica \in Replicas:
            \/ ExecuteInsert(replica)
            \/ ExecuteMerge(replica)
            \/ BecomeLeader(replica)

    ClientAction ==
        \/ Insert
        \/ Select

Так же как и другие рассмотренные алгоритма, ClickHouse работает в асинхронной модели, где узлы могут отказывать и рестартовать.

В спеке явно промоделированы рестарты:

    ReplicaBecomeInactive ==
        /\ \E replica \in Replicas :
          /\ IsActive(replica)
          /\ RepicaStartInactive(replica)
          ...

    ReplicaBecomeActive ==
        /\ \E replica \in Replicas :
          /\ ~IsActive(replica)
          /\ RepicaStartActive(replica)
          ...

Вся информация о реплике хранится в спецальной znode. Так что после рестарта реплика восстанавливает состояние, которое у нее было.

Недетерминизм в системе проявляется в нескольких моментах:

Клиент может послать свой запрос на любую из реплик. Это моделируется в действиях для клиента:

    QuorumReadLin ==
        ...
        /\ \E replica \in Replicas :
            /\ IsActive(replica)
            /\ Cardinality(GetCommitedId) = Cardinality(replicaState[replica].local_parts \ ({quorum.data} \cup GetData(failedParts)))
        ...

 Так же, чтобы описать, что любая реплика может совершить действие в произвольный момент, воспользуемся аналогичной конструкцией в действие для реплик.

     ReplicaAction ==
         \E replica \in Replicas:
          \/ IsActive(replica)
              /\ \/ ExecuteLog(replica)
                 \/ UpdateQuorum(replica)
                 \/ EndQuorum(replica)
                 \/ BecomeInactive(replica)
                 \/ FailedQuorum(replica)
          \/ ~IsActive(replica)
              /\ BecomeActive(replica)

### Тестирование
Проверим, что инвариант в спеке про обрезку лога

    ValidLogPointer == [] (\A replica \in Replicas: IsActive(replica) => deletedPrefix < replicaState[replica].log_pointer)

обнаруживает, когда удалили записи из лога у активной реплики.

Для этого воспользуемся мутационным тестированием и изменим на

    ClearOldLog ==
        /\ Len(log) > 0
        /\ deletedPrefix' = Max({Min(GetLogPointers), deletedPrefix})
        ...

        ClearOldLog ==
            /\ Len(log) > 0
            /\ deletedPrefix' = Max({Max(GetLogPointers), deletedPrefix})
            ...

То есть удаляем не по минимальную запись у активных реплик, а по максимальную.

Запустив TLC обнаружим, что инвариант не выполняется.

Отдельного внимания заслуживает спека про Кворумы.

Одниой из главных проверок служит то, что история будет линеаризуема.

Для этого мы по определению проверим все действия в истории.

Чтение отдает результат, только если у реплики есть все куски, который были втсавленны с кворумом:

    QuorumReadLin ==
        /\ Len(log) > 0
        /\ Cardinality(GetSelectFromHistory(history)) < HistoryLength
        /\ \E replica \in Replicas :
            /\ IsActive(replica)
            /\ Cardinality(GetCommitedId) = Cardinality(replicaState[replica].local_parts \ ({quorum.data} \cup GetData(failedParts)))
            /\ LET max_data == Max(replicaState[replica].local_parts \ ({quorum.data} \cup GetData(failedParts)))
               IN /\ max_data # NONE
                  /\ ReadEvent(GetIdForData(max_data, log))
        /\ UNCHANGED <<log, replicaState, nextRecordData, quorum, lastAddeddata, failedParts>>


Но в КХ есть проблема, что во время чтения znode про максимальный блок, который мы вставили не происходит sync у узлов ZooKeeper-а, таким образом, мы могли получить устаревшее значение этого параметра. Если реплика, на которую мы отправили запрос, пошла на узел в ZooKeeper, который не подтянул все изменения.

Значит, мы можем получить вообще любое значение параметра про послелдний добавленный кусок.

Это мы промоделируем действием:

    QuorumReadWithoutLin ==
        /\ Len(log) > 0
        /\ Cardinality(GetSelectFromHistory(history)) < HistoryLength
        /\ \E replica \in Replicas :
            /\ IsActive(replica)
            /\ LET max_data == Max(replicaState[replica].local_parts \ ({quorum.data} \cup GetData(failedParts)))
               IN /\ max_data # NONE
                  /\ ReadEvent(GetIdForData(max_data, log))
        /\ UNCHANGED <<log, replicaState, nextRecordData, quorum, lastAddeddata, failedParts>>

Если запустить проверку линеаризуемости с ним, то TLC выдаст о нарушении инварианта.
