## Общие слова о системе
ClickHouse - столбцовая система управления базами данных (СУБД) для онлайн обработки аналитических запросов (OLAP). Которая разрабатывается Яндексом и open-source сообществом.

1) Модель данных КХ оперирует таблицами. Каждая таблица реализуется определенным движком, который отвечает за механизм хранения данных и процесс обработки клиентских запросов
2) КХ поддерживает горизонтальное масштабирование с помощью распределенных шардированных таблиц, которые реализуются движком "Distributed"
3) КХ - отказоустойчивая система, где каждый шард распределенной таблицы независимо реплицируется, протокол репликации инкапсулирован в семействе движков "Replicated"
4) Сама репликация реализована с помощью ZK: реплики упорядочивают апдейты (insert, merge) через общий лог в ZK и последовательно применяют их к своему локальному состоянию.

### Replication
Для механизма репликации КХ использует ZooKeeper. В ZooKeeper хранится вся информация о апдейтах (вставки, мерджи).

В Zookeeper хранится лог о вставках и мерджах в системе (хранится только метаинформация). Когда на реплику приходит запрос о вставке, она получает уникальный id для этого блока, пишет к себе на диск данные и записывает информацию о нем в лог. При выполнении лога реплики читают запись и ее тип. Если это вставка, то они идут на реплику, где есть нужный кусок и скучивают его.

Отдельного внимания заслуживают мерджи:

Они должны происходить на всех репликах одинаково, так как, если порядок был разные, то это может привести к невозможности скачать кусок данных.

Для этого последовательность слияний должна быть одинаковая на всех репликах.

ЧТобы этого достичь ClickHouse испольщует лидера, который назначает мерджи.

### Truncate
Невозможно хранить слишком большой лог в ZooKeeper.

Мы можем удалить старые записи, которые обработали все реплики, так как существующие реплики эти записи уже никогда не увидят, а новые при старте будут копировать состояние с активных реплик.

Но надо правильно обрабатывать ситуацию, когда реплика была неактивной и нужные ей записи уже удалили из лога, а потом она снова восстановила связь с ZooKeeper-ом.

Решение: Во время очитски лога мы берем все активные реплики у удаляем по их минимальный log_pointer. Если мы удалил запись у неактивной реплики, то мы создаем znode is_lost в ZooKeeper для нее. Когда реплика снова станет активной, она поймет, что у нее невалидный log_pointer и скопирует себе состояние активной реплики.

### Quorum Insert/Select
Еще одним алгоритмом для верифицирования был выбран алгоритм кворумной записи и чтения.

КХ поддерживает 2 модели согласованности:
* eventual consistency
* "inearizability"

Со второй в ходе верификации были выявлены нарушения.

Для Линеаризуемости КХ использует чтения и вставка с кворумом.

Вставка: Клиент может выбрать сколько реплик должно скопировать себе вставленный кусок, чтобы ему подтвердили вставку. Для этого процесса в ZooKeeper есть специальная znode, в которой записана информация о текущем кворуме: кол-во реплик, которые уже скопировали себе кусок, номер последнего вставленного куска.

Когда кол-во реплик нараблось, то znode с информацией о текущем кворуме удаляется из ZooKeeper, а znode с информацией о последнем вставленном значении обновляется. Если в момент репликации кусок становится невозможно скачать, например, когда реплика, которая вставила стала неактивной, то он помещается в znode "failed_parts".

Во время чтения можно включить также настройку sequential_consistency, чтобы читать данные с реплики со св-вом линеаризуесомти. Для этого в момент чтения реплика, на которую прилетел запрос идет в znode и забирает номер последнего втсавленного куска. И если у нее есть все блоки меньшего номера, которые не лежат в failed  parts, то она отвечает на запрос пользователя, если их нет, то она отвечает, что пока не может обработать этот запрос.

## Уровень абстракции и участники
Определилмся с уровнем абстракции

Система очень похожа на Kafka. Так как почти вся синхронизация проиходит внутри ZooKeeper-а. И реплики между собой почти не взаимодействуют, они только пересылает куски с данными между собой. Моделировать эти общения не надо, в отличие от Kafka, так как в ставке принимает участие ZooKeeper. Будем полагать, что система получила кусок данных, когда прочитала информацию об ставке их в ZooKeeper.

Все связь происходит через ноды в ZooKeeper-е, считаем, что znode - это атомарная отказоустойчивая распределенная ячейкой памяти.

    VARIABLES
        \* Log in zookeeper.
        log,

        \* State replica. Active or not and e.t.c
        replicaState

## Участники системы
Основными действующими лицами у нас будут реплики и клиент. Для лучшей читаемости кода объеденим действия этих сущностей вместе.

    ReplicaAction ==
        \E replica \in Replicas:
            \/ ExecuteInsert(replica)
            \/ ExecuteMerge(replica)
            \/ BecomeLeader(replica)

    ClientAction ==
        \/ Insert
        \/ Select

## Сбои
Так же как и другие рассмторенные алгоритма, ClickHouse работает в асинхронной сети, где узлы могут отказывать и рестартовать.

Заметим, что в спеке явно промоделированы рестарты:

    ReplicaBecomeInactive ==
        /\ \E replica \in Replicas :
          /\ IsActive(replica)
          /\ RepicaStartInactive(replica)
          ...

    ReplicaBecomeActive ==
        /\ \E replica \in Replicas :
          /\ ~IsActive(replica)
          /\ RepicaStartActive(replica)
          ...

Вся информация о реплике хранится в спецальной znode. Так что после рестарта реплика восстанавливает состояние, которое у нее было поледним.

## Недетерминизм
Недетерминизм в системе проявляется в нескольких моментах:

Клиент может послать свой запрос на любую из реплик. Это моделируется в экшенах для клиента:

    QuorumReadLin ==
        ...
        /\ \E replica \in Replicas :
            /\ IsActive(replica)
            /\ Cardinality(GetCommitedId) = Cardinality(replicaState[replica].local_parts \ ({quorum.data} \cup GetData(failedParts)))
        ...

 Так же, чтобы описать, что любая реплика может совершить действие в произвольный момент, воспользуемся аналогичной конструкцией в экшене для реплик.

     ReplicaAction ==
         \E replica \in Replicas:
          \/ IsActive(replica)
              /\ \/ ExecuteLog(replica)
                 \/ UpdateQuorum(replica)
                 \/ EndQuorum(replica)
                 \/ BecomeInactive(replica)
                 \/ FailedQuorum(replica)
          \/ ~IsActive(replica)
              /\ BecomeActive(replica)

## Конечная спека
Для того, чтобы протестировать систему надо сделать конечным число действий в системе и кол-во участников.

Мы ограничим возможное кол-во реплик, которые будут в системе, так же нало ограничить кол-во запросов от польщователя. Для этого воспользуемся приемом из спеки SI и объявисм множество уникальных id для вставок.

    CONSTANTS
        Replicas,
        RecordsId,
        ...

Чтобы TLC не обнаруживал дедлок заведем экшен, в котором будем проверять, что id у всех записей в логе совпадают с этим множеством.

    LegitimateTermination ==
        /\ GetIds(log) = RecordsId
        ...

В спеке Кворум клиент можеть выполнять не только вставки, но и selecet из нашей БД. Поэтому для них мы тоже заведем аналогичный сет. И проверку для его:

LegitimateTermination ==
    /\ GetIds(failedParts) \cup GetCommitedId = RecordsId
    /\ Cardinality(GetSelectFromHistory(history)) = HistoryLength
    ...

### Тестирование

Unit-тестирование будет уместно испольщовать в спеке про кворум, так как там есть операторы, которые вычленяют из истории нужные нам действия.

Приведем пример истории и убедимся, что операторы работают правильно:

    H == <<[type |-> "StartInsert", record_id |-> 1],
     [type |-> "Read", record_id |-> 2],
     [type |-> "FailedInsert", record_id |-> 3],
     [type |-> "EndInsert", record_id |-> 4],
     [type |-> "EndInsert", record_id |-> 5],
     [type |-> "Read", record_id |-> 6]>>

    Unit_test == GetSelectFromHistory(H) = {[type |-> "Read", record_id |-> 2], [type |-> "Read", record_id |-> 6]}

Проверим, что инвариант в спеке про обрезку лога

    ValidLogPointer == [] (\A replica \in Replicas: IsActive(replica) => deletedPrefix < replicaState[replica].log_pointer)

обнаруживает, когда удалили записи из лога у активной реплики.

Для этого воспользуемся мутационным тестированием и изменим на

    ClearOldLog ==
        /\ Len(log) > 0
        /\ deletedPrefix' = Max({Min(GetLogPointers), deletedPrefix})
        ...

        ClearOldLog ==
            /\ Len(log) > 0
            /\ deletedPrefix' = Max({Max(GetLogPointers), deletedPrefix})
            ...

То есть удаляем не по минимальную запись у активных реплик, а по максимальную.

Запустив TLC обнаружим, что инвариант не выполняется.

Отдельного внимания заслуживает спека про Кворумы.

Одниой из главных проверок служит то, что история будет линеаризуема.

Для этого мы по определению проверим все действия в истории.

Чтение отдает результат, только если у реплики есть все куски, который были втсавленны с кворуом

    QuorumReadLin ==
        /\ Len(log) > 0
        /\ Cardinality(GetSelectFromHistory(history)) < HistoryLength
        /\ \E replica \in Replicas :
            /\ IsActive(replica)
            /\ Cardinality(GetCommitedId) = Cardinality(replicaState[replica].local_parts \ ({quorum.data} \cup GetData(failedParts)))
            /\ LET max_data == Max(replicaState[replica].local_parts \ ({quorum.data} \cup GetData(failedParts)))
               IN /\ max_data # NONE
                  /\ ReadEvent(GetIdForData(max_data, log))
        /\ UNCHANGED <<log, replicaState, nextRecordData, quorum, lastAddeddata, failedParts>>


Но в КХ есть проблема, что во время чтения znode про максимальный блок, который мы вставили не происходит sync у узлов ZooKeeper-а, таким образом, мы могли получить устаревшее значение этого параметра. Если реплика, на которую мы отправили запрос, пошла на узел в ZooKeeper, который не подтянул все изменения.

Значит, мы можем получить вообще любое значение параметра про послелдний добавленный кусок.

Это мы промоделируем экшеном

    QuorumReadWithoutLin ==
        /\ Len(log) > 0
        /\ Cardinality(GetSelectFromHistory(history)) < HistoryLength
        /\ \E replica \in Replicas :
            /\ IsActive(replica)
            /\ LET max_data == Max(replicaState[replica].local_parts \ ({quorum.data} \cup GetData(failedParts)))
               IN /\ max_data # NONE
                  /\ ReadEvent(GetIdForData(max_data, log))
        /\ UNCHANGED <<log, replicaState, nextRecordData, quorum, lastAddeddata, failedParts>>

Если запустить проверку линеаризуемости с ним, то TLC выдаст о нарушении инварианта.
