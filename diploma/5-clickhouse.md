## Общие слова о системе
ClickHouse - колоночная распределенная база данных.

Устройство КХ:
* Каждая таблица описывается движком. Движок инкапмулирует механизмы работы хранения данных и обработки.
* Для горизонтального масштабирования КХ поддерживает распределенное шардирование таблиц
* Для отказоуйстойчивости каждый шард реплицируется

### Replication
Для механизма репликации КХ использует ZooKeeper. В ZooKeeper хранится вся информация о апдейтах (вставки, мерджи).

В Zookeeper хранится лог о вставках и мерджах в системе (хранится только метаинформация). Когда на реплику приходит запрос о вставке, она получает уникальный id для этого блока, пишет к себе на диск данные и записывает информацию о нем в лог. При выполнении лога реплики читают запись и ее тип. Если это вставка, то они идут на реплику, где есть нужный кусок и скучивают его.

Отдельного внимания заслуживают мерджи:

Они должны происходить на всех репликах одинаково, так как, если порядок был разные, то это может привести к невозможности скачать кусок данных.

Для этого последовательность слияний должна быть одинаковая на всех репликах.

В ClickHouse используется лидер, который выбирается с помощью ZooKeeper. Лидер по своим данным назначает мерджи и пишет информацию о них в лог.

### Truncate
Невозможно хранить слишком большой лог в ZooKeeper.

Мы можем удалить старые записи, которые обработали все реплики, так как существующие реплики эти записи уже никогда не увидят, а новые при старте будут копировать состояние с активных реплик.

Но надо правильно обрабатывать ситуацию, когда реплика была неактивной и нужные ей записи уже удалили из лога, а потом она снова восстановила связь с ZooKeeper-ом.

Решение: Во время очитски лога мы берем все активные реплики у удаляем по их минимальный log_pointer. Если мы удалил запись у неактивной реплики, то мы создаем znode is_lost в ZooKeeper для нее. Когда реплика снова станет активной, она поймет, что у нее невалидный log_pointer и скопирует себе состояние активной реплики.

### Quorum Insert/Select
Еще одним алгоритмом для верифицирования был выбран алшгоритм кворумной записи и чтения.

КХ поддерживает 2 модели согласованности:
* eventual consistency
* "inearizability"

Со второй в ходе верификации были выявлены нарушения.

Для Линеаризуемости КХ использует чтения и вставка с кворума.

Вставка: Клиент может выбрать сколько реплик должно скопировать себе вставленный кусок, чтобы ему подтвердили вставку. Для этого процесса вZooKeeper есть специальная znode, в которой записана информация о текущем кворуме: кол-во реплик, которые уже скопировали себе кусок, номер последнего вставленного куска.

Когда кол-во реплик нараблось, то znode с информацией о текущем кворуме удаляется из ZooKeeper, а znode с информацией о последнем всиавеоннрмс  в ZK записывается последний номер вставленного куска. Если кусок невозможно скачать (например, когда реплика, которая вставила стала неактивной), то он помещается в failed_parts (znode в зукипере с набором id)

Во время чтения можно включить также настройку sequential_consistency, чтобы читать данные с реплики со св-вом линеаризуесомти. Для этого в момент чтения реплика, на которую прилетел запрос идет в znode и забирает номер последнего втсавленного куска. И если у нее есть все блоки меньшего номера, которые не лежат в failed  parts, то она отвечает на запрос польщователя, если их нет, то она отвечает, что пока не может обработать этот запрос


## Уровень абстракции

Первым делом надо определиться с уровнем абстракции, участниками сети.

Система очень похожа на Kafka. Так как вся синхронизация проиходит внутри ZooKeeper-а. И реплики между собой почти не взаимодействуют.

Моделировать системы будет с внешней сороны, то есть такой, какой ее видит польщователь. Основными действующими лицами у нас будут реплики. Для лучшей читаемости кода объеденим действия всех реплик вместе.

    ReplicaAction ==
        \E replica \in Replicas:
            \/ ExecuteInsert(replica)
            \/ ExecuteMerge(replica)
            \/ BecomeLeader(replica)

Еще у нас есть клиент, который выполняет вставки и чтения в нашей системе. Для него мы создадим отдельный экшен.

    ClientAction ==
        \/ Insert
        \/ Select

## Моделирование сети
Так же как и другие рассмторенные алгоритма, КХ работает в асинзронной сети, где узлы могут отказывать и рестартовать (Византийских отказов - нет).

Недетерминизм в системе проявляется в нескольких моментах:

Клиент может послать свой запрос на любую из реплик. Промоделируем это с помощью оператора \E:

    QuorumReadLin ==
        /\ Len(log) > 0
        /\ Cardinality(GetSelectFromHistory(history)) < HistoryLength
        /\ \E replica \in Replicas :
            /\ IsActive(replica)
            /\ Cardinality(GetCommitedId) = Cardinality(replicaState[replica].local_parts \ ({quorum.data} \cup GetData(failedParts)))
            /\ LET max_data == Max(replicaState[replica].local_parts \ ({quorum.data} \cup GetData(failedParts)))
               IN /\ max_data # NONE
                  /\ ReadEvent(GetIdForData(max_data, log))
        /\ UNCHANGED <<log, replicaState, nextRecordData, quorum, lastAddeddata, failedParts>>

 Нас больше всего интересует часть

     \E replica \in Replicas

 Она отвечает за недетерминизм выбора реплик.

 Так же, чтобы описать, что любая реплика может совершить одно из действий. Воспользуемся аналогичной конструкцией в экшене для реплик.

     ReplicaAction ==
         \E replica \in Replicas:
          \/ IsActive(replica)
              /\ \/ ExecuteLog(replica)
                 \/ UpdateQuorum(replica)
                 \/ EndQuorum(replica)
                 \/ BecomeInactive(replica)
                 \/ FailedQuorum(replica)
          \/ ~IsActive(replica)
              /\ BecomeActive(replica)

 Обмена сообщениями между репликами не происходит. Все связь происходит через ноды в ZooKeeper-е. Аналогию можно провести со спекой Kafka. Там znode считают атомарной отказоустойчивой распределенной ячейкой памяти. .

     VARIABLES
         \* Log in zookeeper.
         log,

         \* State replica. Active or not and e.t.c
         replicaState

 Отказ реплик явно моделировать не надо, так как TLC сама проверяет исполнения, где какая-нибудь реплика отказала. Но у каждой реплики есть эфимерная нода is_active, которая отвечает, что реплика имет связь с зукипером. Как только связь теряется, нода пропадает. Мы промоделируем это в структуре replicaState.

 Основными компонентами в нашей системе являются реплики и лог. Реплики - это конечный сет уникальных id, из которого мы выбираем в экшене ту, которая будет выполнять дейтсвие. Log целиком зранится в зукипере и в него приходит информация о новых вставках, мерджах и т.д. Лог логично сделать typle.

 Каждую спеку мы сделаем конечной, для этого мы добавим RecordsId - это сет с уникальными id (для этого мы используем module value). Во время вставки в лог новой записи мы патаемся для нее получить id-ник (Аналогично в мехвнизме КХ каждой новой записи в логе выдается уникальный индентификатор). И если его нет, то это означает, что произвелось максимальное кол-во вставок. Вместе с этим у нас возникает проблема, что TLC начинает обнаруживать дедлоки в нашей спеке, чтобы такого не было определим экшен который будет означать о корректном завершении алгоритма и поможет TLC различать делок спеки от дедлока алгоритма.

 *КОД*

 В алгоритме репликации КХ для каждой реплики в ZK создана нода, где хранится информация о ней: флаги is_active, log_pointer(какую следующую запись должна выполнить реплика), информация о кусках, коьорые есть на реплике. Для моделирования этого мы используем структуру replicaStates, в которой для каждой из реплик поддерживаем информацию о ней в зукипере. Заметим, что в КХ у кажджой реплики есть еще очередь текущих заданий, которая тоже хранится в зукипере и данные из лога попадают сначала в очередь, но мы опустим этот механизм, так как он не влияет на проверку наших мехнизмов. Считаем, что когда реплика делает FetchLog, то она скачивает себе кусок нужный с другой реплики и обновляет свое состояние в ZK

 *КОД*

 Первым и гланым инвариантом, которым должна система удовлетворять - это TypeOK. Для лога мы хотим, чтобы все записи в текущий момент в логе уловлетворяли шаблону

 ```
 LogRecords == [data: Nat, id: RecordsId]
 LogTypeOK == Range(log) \subseteq LogRecords
```

Для реплики мы описали все поля, которые могут быть и все их значения.


## Replication
### Алгоритм
В Zookeeper хранится лог о вставках и серджах в системе (хранится только метаинформация). Когда на реплику приходит запрос о вставке, она получает уникальный id для этого блока, пишет к себе на диск данные и записывает информацию о нем в лог. При выполнении лога реплики читают запись и ее тип. Если это вставка, то они идут на реплику, где есть кусок нужный и скуачиаают его себе.

В сиситеме избегают большую фрагментацию тем, что мерджат блоки с данными. Так как мы хотим, чтобы на всех репликах была однотипная структура мерджа (чтобы было удобно скачивать куски нужные), то последовательность слияни должна быть одинаковая. Для этого существует лидер, который выбирается с помощью ZooKeeper-а. Лидер по своим данным назначает мерджи и пишет информацию о них в лог.

### Перевод в КХ
Аналогично спеки Kafka текущий лидер - это просто VARIABLES leader, которая хранится в Znode в зукипере.

С новой ролью появлись экшены для лидера.

ExpiredLeader - реплика в КХ может потерять свзяь с зукипером или отказать, и мы должны перевыбрать новго лидера. Так же смена лидера происходит, когда реплика сильно тормозит и находится далеко от свежих записей в логе.

LeaderCreateMerge - этот экшен назвначает новый мердж. Реплика смотрит на куски, которые на ней есть и выбирает 2 из них, которые она смерджит.

Чтобы рассматривать конечные исполнения спеки, воспользуемся приемом из SI и будем проверять в экшене, что в логе есть все id для записей

Честность отдадим LeaderAction, чтобы у нас назначались новые мерджи.

Проверять будем св-во, что у всех реплик в конечном итоге одинаковые части. То есть наш алгоритм в конечном итоге давет одинаковое состояние на всех репликах

*КОД*

## Truncate
### Алгоритм
Описание алгоритма: у нас есть лог в зукипере, мы не хотим хранить его весь, так как старые записи, которые обработали все реплики нам хочется удалить, так как в зукипере нельзя хранить б.м. нод. Удалить мы их можем, так как существующие реплики эти записи уже никогда не увидят, а новые реплики при старте будут копировать себе состояние с активных реплик

Проблема, которая возникает в этом алгоритме в том, что реплики могут стать неактивными, так как хранить весь лог для них мы не хотим, так как такие репилики могут быть навсегда отключены. Нам надо правильно обрабатывать ситуацию, когда реплика после восстановления свзи с зукипером увиделоаЮ, что ее log_pointer не валиден.

Решение: Во время очитски лога мы берем все активные реплики у удаляем по минимальную запись, на которую они ссылаются. Так мы поддерживаем инвариант, что у любой активной реплики валидный log_pointer. Если мы удалил запись у неактивной реплики, то мы ставим ей флаг is_lost. Когда реплика снова станет активной, она поймет, что у нее невалидный log_pointer и скопирует себе состояние активной реплики.

### Перевод в КХ
VARIABLES: deletedPrefix - для номера удаленных записей мы не хотим менять лог, а будем поддерживать deletedPrefix. Это номер записей в логе до которой мы все удалили

Для TypeOK будем проверять, что на каждом шаге deletedPrefix в натуральных числах

Основные экшены:
* ReplicaBecomeInactive и ReplicaBecomeActive - экшены, которые отвечают за то, что реплика теряет и восстанавливает связь с зукипером
* ExecuteLog - Чтобы выполнить запись в логе надо выбрать реплику, которая активна и не is_lost, проверить, что есть запись, которую мы можем забрать себе из лога.
* Insert - действие клиента. Нам надо для вставки выбрать активную реплику и взять id-ник, который мы еще не испольщовали и добавить запись в лог.
* ClearOldLog - при очистке лога мы выбираем минимальную запись по которую надо удалить лог и у неактивных реплик, чей log_pointer меньше, обновлячем флаг is_lost
* CloneReplica - Выбираем две реплики одну умершую и другую аквтиную и копируем состояние.

Ограничение спеки: Экшен для, который означает, что использованы все id-ники. Просто берем все id из лога и сравниваем их с изначальным set-ом

*КОД*

Next: Почему мы Next не разбили на несколько действий? (Действия пользователя или реплики). Тут это сделано специально, так как нам надо навесить честность на некоторые действия, чтобы гарантировать, что лог почистится.

Честность: Мы хотим рассматривать поведения только те, где лог будет удаляться. Для этого нам надо навесить честность н действия, которые обновляют лог и заставляют реплики его выполнять

Проверки: Мы хотим проверить инвариант, что у всех активных реплик log_pointer всегда находится в валдиных записях

Основным инвариантом считаем ValidLogPointer, который проверяет, что на каждом шаге у активной реплики валидный log_pointer.

*КОД*

Так же мы хотим проверить, что эьтот инварант действительно проверяет что-то правильное. Воспользуемся мутационным тестированием и поменяем ClearOldLog

ClearOldLog ==
    /\ Len(log) > 0
    /\ deletedPrefix' = Max({Max(GetLogPointers), deletedPrefix})
    /\ replicaState' = [replica \in Replicas |-> [is_active |-> replicaState[replica].is_active,
                                                  is_lost |-> InvalidLogPointer(replica),
                                                  log_pointer |-> replicaState[replica].log_pointer,
                                                  local_parts |-> replicaState[replica].local_parts]]
    /\ UNCHANGED <<log, nextRecordData>>

Теперь мы берем не минимум из log_pointer, а максимум. В какой-то момент мы удалим записи для активных реплик. При запуске TLC получаем трейс, где наш придикат нарушился.

IsLogCleared - помагает понять, что мы правильно расставили честность, так как если он выполняется, то в любом исполнении теперь будет момент, когда мы почистим лог

## Quorum
### Алгоритм
КХ поддерживает вставку и чтение с кворумов.

Вставка: Клиент может выбрать сколько реплик должно скопировать себе вставленный кусок, чтобы ему потвердили вставку. Для этого процесса в ZK есть специальная znode, в оторой записан текущий кворум (кол-во реплик, которые уже скопировали себе кусок и другая метаинформация). Когда кол-во реплик нараблось, то это znode удаляется из ZK и в ZK записывается последний номер вставленного куска. Если кусок невозможно скачать (например, когда реплика, которая вставила стала неактивной), то он помещается в failed_parts (znode в зукипере с набором id)

Во время чтения можно включить также настройку sequential_consistency, чтобы читать данные с реплики со св-вом линеаризуесомти. Для этого в момент чтения реплика, на которую прилетел запрос идет в znode и забирает номер последнего втсавленного куска. И если у нее есть все блоки меньшего номера, которые не лежат в failed  parts, то она отвечает на запрос польщователя, если их нет, то она отвечает, что пока не может обработать этот запрос

###  Перевод в КХ
В  VARIABLES добавились состояния кворума (с кол-вом реплик), максимально вставленный номер lastAddeddata и failedParts (мн-во с зафелившимися блоками)

Так же есть переменная history, в ней хранится история действий вставок и чтений. Ее мы используем, чтобы проверять свойства алгоритма. У нас есть 4 типа вставок - Начало записи, конец записи/аборт записи. Запись растягивается во времени, так как мы ждем, пока нужное кол-во реплик скопирует себе вставленный кусок. И есть чтение, которое выполняется мн-но, считыванием znode с номером последнего вставленного блока

ExecuteLog - экшен, когда реплика просто выполняет лог и не участвует в кворуме текущем

UpdateQuorum - в этом экшене, если реплика еще не скопировала нужный себе кусок, то она выполняет транзацию с обновление корума и скачиванием себе куска данных

FailedQuorum - если реплика хочет поучаствовать в кворуме, но другие реплики у которых есть этот кусок не активны, то кворум надо считать проваленым и обнулить информацию

EndQuorum - Если реплика понимает, что после нее набралось нужное кол-во реплик, то она должна обновить информацию о последнем вставленном блоке с кворумом и обнкдить информацию о текущем кворумею (В одной транзакции)

Мы хотим проверить, что вставка и чтение с кворумом удовлетворяет linearizability. Для этого промоделируем экшен, где мы можем читать только тогда, когда у нас текущее состояние системы. (Мы просто смотрим, что у нас есть все записи, которые есть в логе и на которых кворум завергился)

НО! В КХ во время запросов к ЗК нет синхронизации внутри реплик в ZK. Таким образом, мы можем пойти на реплику, у которое старое значение znode с посдледним вставленным блоком. За это отвечает экшен QuorumReadWithoutLin.

Мы хотим проверить св-во linearizability по истории, как это делано в SI. Мы полуемся определнием проверяем, что история удовлетворяет последовательной. Если испольщовать QuorumReadLin, то проверка проходит, но если испольщовать экшен QuorumReadWithoutLin, то история получается нелианирезуемой.
