# Часть 1 (modeling)
### Реактивность
В отличие от классических алгоритмов, которые получают вход, обрабатывают его за конечное время и выдают ответ, распределенные системы реактивны: они работают в бесконечном цикле, реагируя на события: поступающие от клиентов запросы, сигналы от таймеров и сообщения, отправляемые по внутреннему протоколу.

Описать такую систему - значит описать ее мгновенное состояние и набор обработчиков различных событий, которые меняют это состояние.

В TLA+ состояние системы (всех составляющих ее узлов и компонентов) мы описываем с помощью набора переменных, а переход между состояниями (реакцию системы на события) - с помощь экшенов.

Например, состояние реплик в Raft включает: currentTerm, state, votedFor.

Примеры экшенов:
- BecomeLeader в Kafka - контроллер назначил узел новым лидером партиции
- FollowerReplicate в Kafka - follower отреплицировал сообщение из лога лидера партиции
- Restart в Raft - реплика по каким-то причинам рестартовала и потеряла данные, хранившиеся в оперативной памяти
- ClientRequest рестарт в Raft - лидер получил запрос от пользователя
- Commit в Snapshot Isolation - транзакция применилась к хранилищу

Поведение всей системы описывается дизъюнкцией обработчиков всех возможных событий.

Приведем в качестве примера спеку Kafka:

    Next ==
        \/ ControllerElectLeader
        \/ ControllerShrinkIsr
        \/ BecomeLeader
        \/ LeaderExpandIsr
        \/ LeaderShrinkIsr
        \/ LeaderWrite
        \/ LeaderIncHighWatermark
        \/ BecomeFollowerTruncateToHighWatermark
        \/ FollowerReplicate

### Уровень абстракции
Распределенная система представляет собой отдельные узлы / микросервисы, которые играют разные роли, и взаимодействуют путем обмена сообщениями. Но внешний пользователь как правило не наблюдает никакой распределенности, узлы системы скрыты от него за сетевым адресом. По этому адресу клиент отправляет команды: Get/Set для k/v хранилища, Create/Append/Delete для файловых систем и т.п. и получает ответ, обычно с помощью клиентской библиотеки. Для пользователя система – это конкурентный объект в модели разделяемой памяти, а сетевой адрес - по сути имя этого объекта.

Пример - взаимодействие клиента с системой ZooKeeper:

    zk = KazooClient()
    zk.start()
    zk.create("/my/favorite/node", b"a value")

Все рассуждения выше можно применить не только с самой системе, но и к отдельным ее компонентам. Например: Kafka использует ZooKeeper для хранения состава кворума. Для узлов-контроллеров состав кворума – это одна разделяемая переменная, но внутри ZK эти данные хранятся в нескольких копиях и синхронизируются с помощью протокола ZAB.

Первый шаг работы над спецификаций системы -  выбор *уровеня детализации* разных компонент.

Сравним существующие системы и их спеки:

Paxos/Raft: В спеках этих алгоритмов выбран самый высокий уровень детализации: моделируется состояние отдельных узлов распределенной системы и протокол их взаимодействия, моделируется отказ некоторых узлов. В системах, где одной из составляющих является обмен сообщениями по сети надо уметь моделировать асинхронность сети.

Kafka: В этой системе лидер принимает новые записи клиента и отправляет их остальным узлам в кворуме. Контроллер используют Zookeeper для упорядочивания апдейтов кворума. Для узлов, которые играют роль контроллера, это разделяемое состояние в shared memory, при этом внутри ZooKeeper - это несколько узлов, которые обмениваются сообщениями. Можно провести аналогию между ZNode в ZooKeeper-е и атомиком в C++. Создатели сами проводят такое сравнение: "Our system, Zookeeper, hence implements an API that manipulates simple wait-free data objects organized hierarchically as in file systems", так как wait-free  - это свойство конкурентных объектов. Вот, что сам автор спеки пишет по этому поводу: "This is the model's equivalent of the state in Zookeeper, but generally we ignore the complexity of Zookeeper itself. Instead we allow simple atomic operation to the state directly within individual actions". Поэтому, это просто переменная и в спеке участники будут ее изменять в действиях. Мы явно моделируем обмен сообщениями во время вставки, поскольку на пути вставки нет ZooKeeper-а, а лидер напрямую отправляет информацию о вставке ISR.

Percolator - это протокол распределенных client-side транзакций над BigTable - распределенных k/v хранилищем. Все взаимодействие между клиентами происходит через Big Table. В этой спеке самый высокий уровень абстракции, в ней нет никакого взаимодействия по сети между узлами системы.

### Моделирование участников
Не всегда надо явно моделировать сущности, которые производят действия в системе.

Вот какой пример приводит Лэмпорт:
"Mathematical manipulation of specifications can yield new insight. A producer/consumer system can be written as the conjunction of two formulas, representing the producer and consumer processes. Simple mathematics allows us to rewrite the same specification as the conjunction of n formulas, each representing a single buffer element. We can view the system not only as the composition of a producer and a consumer process, but also as the composition of n buffer-element processes. Processes are not fundamental components of a system, but abstractions that we impose on it. This insight could not have come from writing specifications in a language whose basic component is the process."

Для распределенных систем можно так же моделировать не самих участников, а сущности, с которыми они работают.

Например, в Paxos у proposer-а внутри бесконечный цикл, в котором он выбирает новый ballot и пытается предложить значение

В спеке proposer-а и явного цикла нет, вместо этого моделируются отдельные proposal-ы.

    Next == \/ \E b \in Ballot : \/ Phase1a(b)
                                 \/ \E v \in Value : Phase2a(b, v)
            \/ \E a \in Acceptor : Phase1b(a) \/ Phase2b(a)

В Snapshot Isolation не моделируются клиенты, а транзакции рождаются "из воздуха"

    Next == \/ \E txn \in TxnId :

                   (* Public actions *)
                \/ Begin(txn)
                \/ Commit(txn)
                \/ ChooseToAbort(txn)
                ...

### Моделирование мира
Займемся моделированием мира, в котором функционирует наша система: опишем, как сеть передает сообщения, как течет время, как отказывают узлы.

В теории распределенных вычислений выделяют две основные модели: *синхронную* и *асинхронную*. В первой нет ограничений на
* время доставки сообщений
* дрейф часов
* относительную скорость работы узлов.

В синхронной модели эти ограничения есть.

Реальность где-то посередине. Например: реальная сеть большую часть времени ведет себя синхронно, но никакая сетевая инфраструктура не может гарантировать это на 100% времени. Бывают периоды нестабильности, когда сеть задерживает и теряет сообщения.

Мы будем использовать асинхронную модель. Мы сильно пессимизируем реальность, настоящая сеть не бывает асинхронной бесконечно долго. С другой стороны, если если мы убедимся в корректности алгоритма или системы в максимально сложной модели, то и в реальном мире получим только корректные поведения. Еще одна причина, по которой выбрана асинхронная модель: моделировать ее гораздо проще.

Поговорим подробнее про каждую из составляющих модели.

### Сеть и отправка сообщений
Сеть, через которую взаимодействуют узлы распределенной системы, – это провода и промежуточные устройства: коммутаторы и роутеры. Сообщение, отправленное в сеть, существует либо в виде сигнала внутри
провода, либо в виде байт в буфере промежуточного устройства.

В TLA мы можем абстрагироваться от этих физических подробностей и представить сеть просто как множество in-flight сообщений:

    VARIABLE msgs

Для отправки сообщения добавляем его в это множество:

    Send(m) == msgs' = msgs \cup {m}

Пример:

Proposer в Single-Decree Paxos отправляет аксепторам сообщение Prepare(b):

    Phase1a(b) == /\ Send([type |-> "1a", bal |-> b])
                  /\ UNCHANGED <<maxBal, maxVBal, maxVal>>

Аксептор получает его и отправляет обратно Promise:

    Phase1b(a) == /\ \E m \in msgs :
                      /\ m.type = "1a"
                      /\ m.bal > maxBal[a]
                      /\ maxBal' = [maxBal EXCEPT ![a] = m.bal]
                      /\ Send([type |-> "1b", acc |-> a, bal |-> m.bal,
                                mbal |-> maxVBal[a], mval |-> maxVal[a]])
                  /\ UNCHANGED <<maxVBal, maxVal>>

В Kafka контроллер меняет назначает нового лидера и новый кворум, после чего отправляет уведомления репликам партиции:

    ControllerUpdateIsr(newLeader, newIsr) == \E newLeaderEpoch \in LeaderEpochSeq!IdSet :
        /\ LeaderEpochSeq!NextId(newLeaderEpoch)
        /\  LET newControllerState == [
                leader |-> newLeader,
                leaderEpoch |-> newLeaderEpoch,
                isr |-> newIsr]
            IN  /\ quorumState' = newControllerState
                /\ leaderAndIsrRequests' = leaderAndIsrRequests \union {newControllerState}

### Сбои сети
Асинхронная сеть может задерживать сообщения на произвольное время, терять их или дублировать или даже распадаться на несвязные сегменты:

Явно моделировать задержки или потери сообщений не нужно: model checker исследует все возможные траектории, в том числе те, в которых сообщения долго не доставляются или вообще не выбираются из множества msgs. Аналогично чекер автоматически проверит сценарии партишенов в сети.

Для моделирования дублирования сетью сообщений достаточно не удалять их из множества msgs.

### Широковещательная рассылка
Большинство распределенных алгоритмов использует не просто точечную отправку сообщений, а широковещательную рассылку (broadcast)
* RequestVote и AppendEntries в Raft
* Prepare и Propose в Paxos

Например, в спеке Paxos в начале первой фазы в сеть помещается единственное сообщение, адресованное всем acceptor-ам сразу, в нем не указывается адресат:

    Send([type |-> "1a", bal |-> b])

В сообщениях, которые приходят от acceptor-а к proposer-у необходимо указывать адресата и отправителя. Отправитель указывается явно, чтобы proposer понимал, когда он соберет кворум ответов от acceptor-ов.

    Send([type |-> "1b", acc |-> a, bal |-> m.bal,
          mbal |-> maxVBal[a], mval |-> maxVal[a]])

Такие техники упрощают алгоритм по сравнению с реальным миром и уменьшают гранулярность событий, так что существует вероятность потерять какие-то сложные сценарии с гонками и надо быть аккуратными.

### Сбои узлов
Существуют 3 основные модели сбоев:
* *Отказ* - узел взорвался и больше никогда не примет участия в системе.
* *Рестарт* - узел не отвечал некоторое время, а потом опять начал участвовать в системе.
* *Византийский* - сбойный узел ведет себя абсолютно произвольным образом - он попал под контроль злоумышленника или же нарушает протокол из-за ошибки в реализации алгоритма.

С моделированием отказов все просто: мы выбрали асинхронную модель, а в ней невозможно отличить умерший узел от узла, который _бесконечно долго не получает отправленные ему сообщения.

В FLP-теореме отказы узлов явно не выражены в графе конфигураций, сбоям соответствуют исполнения, в которых не будет состояний, связанных с умершим узлом. В TLA+ аналогично model checker исследует такие состояния, где узел может не отвечать.

Алгоритм, устойчивый к рестартам узлов, должен хранить часть своего состояния в надежном персистентном хранилище.

Например, в RAFT реплика в фазе выбора лидера обязана надежно сохранять отданный голос перед ответом кандидату на его запрос RequestVote, в противном случае после рестарта она может проголосовать в том же терме во второй раз.

В TLA+ можно промоделировать рестарт узла отдельным экшеном, который сбрасывает его волатильное состояние

В Basic Paxos аксептор после Promise(b) на первой фазе должен игнорировать / отклонять сообщения с меньшими номерами.
1) Для этого аксептор запоминает максимальный ballot number, который он получал от proposer-ов.
2) Запоминать этот номер нужно обязательно в персистентом хранилище перед отправкой Promise / Accept, чтобы после рестарта не забыть о данном обещании.

В Raft рестарт реплики моделируется с помощью действия Restart

    Restart(i) ==
    /\ state'          = [state EXCEPT ![i] = Follower]
    /\ votesResponded' = [votesResponded EXCEPT ![i] = {}]
    /\ votesGranted'   = [votesGranted EXCEPT ![i] = {}]
    /\ voterLog'       = [voterLog EXCEPT ![i] = [j \in {} |-> <<>>]]
    /\ nextIndex'      = [nextIndex EXCEPT ![i] = [j \in Server |-> 1]]
    /\ matchIndex'     = [matchIndex EXCEPT ![i] = [j \in Server |-> 0]]
    /\ commitIndex'    = [commitIndex EXCEPT ![i] = 0]
    /\ UNCHANGED <<messages, currentTerm, votedFor, log, elections>>


В этом примере персистентное состояние (currentTerm, votedFor, log) помещается в UNCHANCHED, волатильное (state, votesResponded, votesGranted ...)

Моделирование византийских отказов требует отдельного изучения и в этой работе затронута не будет

### Время
Узлы в распределенных системах используют время для таймаутов, а таймауты - для обнаружения сбоев узлов.

Если за определенный период узел не получил ответ от другого, то он считает его умершим.

Примеры:

В Raft follower ждет определенное время (election timeout) сообщений от текущего лидера, и если не получает их, то переходит в новый терм и инициирует выборы нового лидера.

В спеке Raft для этого таймаута заведен отдельный экшн:

    Timeout(i) == /\ state[i] \in {Follower, Candidate}
                  /\ state' = [state EXCEPT ![i] = Candidate]
                  /\ currentTerm' = [currentTerm EXCEPT ![i] = currentTerm[i] + 1]
                  \* Most implementations would probably just set the local vote
                  \* atomically, but messaging localhost for it is weaker.
                  /\ votedFor' = [votedFor EXCEPT ![i] = Nil]
                  /\ votesResponded' = [votesResponded EXCEPT ![i] = {}]
                  /\ votesGranted'   = [votesGranted EXCEPT ![i] = {}]
                  /\ voterLog'       = [voterLog EXCEPT ![i] = [j \in {} |-> <<>>]]
                  /\ UNCHANGED <<messages, leaderVars, logVars>>

Никаких часов при этом явно не моделируется.

### Недетерминиз
Недетерминизм является одним из главных источников сложности проектирования распределенных систем. Недетерминизм возникает естественным образом из-за реактивной природы распределенных систем: они реагируют на внешние воздействия (запросы клиентов) и подвержены влиянию неконтролируемой среды (сеть, течение времени), железа / runtime-а  (паузы gc) и сбоев.

В TLA+ есть два основных способа выразить недетерминизм:

С помощью квантора \E можно выбрать, какой из узлов системы следующим сделает "ход": получит сообщение, перезагрузится и т.п.

С помощью дизъюнкции в экшенах: например, клиент может отправить на узел БД либо запрос SELECT, либо INSERT:

    ClientAction == Insert \/ Select

Примеры:

В спеке Raft-а выбирается какое сообщение будет доставлено или какой из узлов выполнит действие

    Next == /\ \/ \E i \in Server : Restart(i)
               \/ \E i \in Server : Timeout(i)
               \/ \E m \in DOMAIN messages : Receive(m)
               \/ \E m \in DOMAIN messages : DuplicateMessage(m)


В спеке Paxos-а выбирается какое сообщение будет обработано или какой из acceptor-ов ответит на запрос

    Phase2b(a) == \E m \in msgs : /\ m.type = "2a"
                                  /\ m.bal \geq maxBal[a]
                                  /\ maxBal' = [maxBal EXCEPT ![a] = m.bal]
                                  /\ maxVBal' = [maxVBal EXCEPT ![a] = m.bal]
                                  /\ maxVal' = [maxVal EXCEPT ![a] = m.val]
                                  /\ Send([type |-> "2b", acc |-> a,
                                           bal |-> m.bal, val |-> m.val])

    Next == \/ \E b \in Ballot : \/ Phase1a(b)
                                \/ \E v \in Value : Phase2a(b, v)
           \/ \E a \in Acceptor : Phase1b(a) \/ Phase2b(a)

В Kafka-е:

    ControllerElectLeader == \E newLeader \in quorumState.isr :
        /\ quorumState.leader # newLeader
        /\ ControllerUpdateIsr(newLeader, quorumState.isr)
        /\ UNCHANGED <<nextRecordId, replicaLog, replicaState>>

В одном действие все действия происходят за один раз

Например:

В действие для acceptor-а в первой фазе алгоритма Paxos

    Phase1b(a) == /\ \E m \in msgs :
                      /\ m.type = "1a"
                      /\ m.bal > maxBal[a]
                      /\ maxBal' = [maxBal EXCEPT ![a] = m.bal]
                      /\ Send([type |-> "1b", acc |-> a, bal |-> m.bal,
                                mbal |-> maxVBal[a], mval |-> maxVal[a]])
                  /\ UNCHANGED <<maxVBal, maxVal>>

acceptor получает сообщение от proposer-а. Проверяет, что ballot у сообщения больше, чем текущий локальный у acceptor-а, и если это так, то он обновляет максимально полученный ballot и отвечает текущему proposer-у.

В этом действие инкапсулированы 3 действия, которые произошли в системе.

Можно провести аналогия с FLP. В док-ве авторы в переходе между состояниями объединяют 3 действия: получение сообщения, изменение состояния и отправку сообщения. Так как граф конфигураций в FLP и в TLA+ однотипны, то можно поступить аналогично и в нашей спеке

В действие TLA происходит все аналогично. Надо очень аккуратно выбрать действия, которые мы будем делать атомарно в спеке. Для распределенных систем это сделать проще, так как почти все внутренние действия узлов для изменения состояния рассматриваются до отправки нового сообщения, и для других участников системы это происходит как будто атомарно.

Для параллельных алгоритмов все сложнее, потому что, если использовать в PlusCal-е один лэйбл (это те действия, которые происходят атомарно) можно потерять гонки, которые происходят в алгоритме, так как мы можем написать в лэбле чтение из одной ячейки и запись в другую.

### Cвойства системы
Требования к системе / алгоритму задаются в виде *свойств*. Свойство – набор траекторий в графе состояний системы

В LTL мы формулируем свойство для одного какого-то исполнения. Была выбрана эта модель, так как нас интересуют высказывания про отдельные исполнения, а не про несколько сразу, так как таких свойств не возникает в распределенных системах.

Нас будут интересовать только 2 типа свойств:
* *Safety свойство* говорит о том, что если свойство нарушено на траектории, то существует префикс, на котором это свойство нарушается и на любом расширении префикса свойство будет нарушено. Эти свойство должны выполняться в каждом состоянии нашей системы.
* *Liveness свойство* говорит о том, что для любого бесконечного поведение существует состояние, в котором свойство выполняется.

При верификации распределенных алгоритмов достаточно проверять только свойства двух приведенных типов: любое свойство можно выразить как пересечение свойств safety и liveness (https://pron.github.io/posts/tlaplus_part3#safety-and-liveness)

Приведем примеры safety свойств:

Пример инварианта в Raft: Election Safety в Raft: в каждом терме может быть выбран не более чем один лидер.
В TLA+ это свойство выражено отрицанием инварианта MoreThanOneLeader

    BothLeader( i, j ) ==
         /\ i /= j
         /\ currentTerm[i] = currentTerm[j]
         /\ state[i] = Leader
         /\ state[j] = Leader

    MoreThanOneLeader ==
        \E i, j \in Server :  BothLeader( i, j )

В Paxos safety-свойство означает, что если предложение (b, v) выбрано (т.е. его приняло большинство аксепторов), то в любом предложении с большим номером может быть только то же самое значение:

    Agreed(v,b) == \E Q \in Quorum2: \A a \in Q: Sent2b(a,v,b)
    NoFutureProposal(v,b) == \A v2 \in Value: \A b2 \in Ballot: (b2 > b /\ Sent2a(v2,b2)) => v=v2

    SafeValue == \A v \in Value: \A b \in Ballot: Agreed(v,b) => NoFutureProposal(v,b)


При проверки этого инварианта мы пользуемся тем, что сообщения из множества msgs не удаляются, а значит в каждом из состояний по нему можно восстановить все выбранные предложения.


В Kafka аппенды, которые подтверждены лидером, должны находиться в логах всех реплик кворума. В спеке TLA+ это свойство StrongIsr:

    StrongIsr == \A r1 \in Replicas :
        \/ ~ ReplicaPresumesLeadership(r1)
        \/ LET  hw == replicaState[r1].hw
           IN   \/ hw = 0
                \/ \A r2 \in quorumState.isr, offset \in 0 .. (hw - 1) : \E record \in LogRecords :
                    /\ ReplicaLog!HasEntry(r1, record, offset)
                    /\ ReplicaLog!HasEntry(r2, record, offset)

Свойство WeakIsr отличается тем, в нем проверяется, что запись присутствует у всех реплик, которые текущая считает своим ISR.

Liveness свойства почти никогда не применяются для распределенных систем, так как о завершении можно говорить, только в случае частично синхронной или синхронной сети, а в TLA+ мы не моделируем их.

Liveness-свойства мы рассматривать не будем:

Репликация в реальных распределенных системах как правило реализуется с помощью решения задачи распределенного консенсуса, а в асинхронной модели обеспечить завершаемость консенсуса невозможно в силу FLP-теоремы.

Если отказаться от консенсуса, то можно добиться завершаемости алгоритмов, но вместе с тем придется ослабить модель согласованности до eventual consistency, но ее мы не рассматриваем из-за слабых гарантий для пользователя.

### Проверка моделей согласованности
В предыдущем параграфе были приведены примеры safety-свойств, если мы смотрим на систему не изнутри, а снаружи, то все алгоритмы от нас скрыты, нам доступно лишь наблюдаемое поведение, которое формулизуется в виде моделей согласованности.

Модели согласованностей для распределенных хранилищ целый диапазон https://jepsen.io/consistency

В большинстве систем моделируют всего 2 модели: eventual consistency и linearizability. Мы рассматриваем именно linearizability, так как оно наиболее понятно пользователю описывает поведение системы и является одной из самых строгих гарантий.

Для описания таких свойств надо иметь историю в каком-нибудь виде, так как они основываются на порядке выполнения операций.

Для проверки linearizability уже существует готовая спека https://github.com/lorin/tla-linearizability, которая по определению проверяет историю на линеаризуемость.
