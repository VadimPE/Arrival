# Часть 1 (modeling)
### Реактивность
В отличие от классических алгоритмов, которые получают вход и за конечное время выдают ответ, распределенные системы реактивны: они в бесконечном цикле реагируют на поступающие от клиентов запросы, сигналы от таймеров и сообщения, отправляемые по внутреннему протоколу.

Описать такую систему -- значит описать ее мгновенное состояние и набор обработчиков различных событий, которые приводят к изменению состояния.

В TLA+ состояние системы (всех составляющих ее узлов и компонентов) мы описываем с помощью набора переменных, а переход между состояниями (реакцию системы на события) - с помощь действий.

Например, состояние реплик в Raft включает:
* currentTerm - номер терма, в котором находится реплик
* state - текущая роль реплики (leader / follower / candidate)
* votedFor - голос, отданный в фазе выбора лидера в текущем терме

Примеры действий:
- BecomeLeader в Kafka - контроллер назначил узел новым лидером партиции
- FollowerReplicate в Kafka - follower отреплицировал сообщение из лога лидера партиции
- Restart в Raft - реплика по каким-то причинам рестартовала и потеряла данные, хранившиеся в оперативной памяти
- ClientRequest в Raft - лидер получил запрос от пользователя
- Commit в Snapshot Isolation - транзакция применилась к хранилищу

Поведение всей системы описывается дизъюнкцией обработчиков всех возможных событий. Приведем в качестве примера спеку Kafka:

    Next ==
        \/ ControllerElectLeader
        \/ ControllerShrinkIsr
        \/ BecomeLeader
        \/ LeaderExpandIsr
        \/ LeaderShrinkIsr
        \/ LeaderWrite
        \/ LeaderIncHighWatermark
        \/ BecomeFollowerTruncateToHighWatermark
        \/ FollowerReplicate

### Уровень абстракции
Распределенная система представляет собой отдельные узлы / микросервисы, которые играют разные роли, которые взаимодействуют путем обмена сообщениями. Но внешний пользователь как правило не наблюдает никакой распределенности, узлы системы скрыты от него за сетевым адресом. По этому адресу клиент отправляет команды и получает ответ, обычно с помощью клиентской библиотеки. Для пользователя система – это конкурентный объект в модели разделяемой памяти, а сетевой адрес -- по сути имя этого объекта.

Пример - взаимодействие клиента с системой ZooKeeper:

    from kazoo.client import KazooClient

    zk = KazooClient(hosts='127.0.0.1:2181')
    zk.start()
    zk.create("/my/favorite/node", b"a value")

Рассуждения выше можно применить не только с самой системе, но и к отдельным ее компонентам. Например: Kafka использует ZooKeeper для хранения состава кворума. Для узлов-контроллеров состав кворума – это одна разделяемая переменная, но внутри ZK эти данные хранятся в нескольких копиях на разных узлах и синхронизируются с помощью протокола ZAB.

Первый шаг работы над спецификаций системы -- выбор уровня детализации состовляющих ее компонент.

Сравним существующие системы и их спеки:

Paxos/Raft: В спеках этих алгоритмов выбран самый высокий уровень детализации: моделируется состояние отдельных узлов распределенной системы и протокол их взаимодействия, моделируются рестарты узлов.

Kafka:

В этой системе лидер партиции принимает новые записи клиента и записывает их на кворум реплик (IRS, *in-sync replicas*). В случае смерти лидера партиции узел-*контроллер* назначает через ZK нового лидера и новый кворум для записи, и отправляет уведомления репликам. ZK нужен для глобального упорядочивания изменений лидера / ISR.

При моделировании протокола репликации Kafka в TLA+ ZK явно не представлен: ISR и лидер моделируются как отдельные переменные, с которыми работает контроллер. Для контроллера ZK - просто отказоустойчивая разделяемая память, вся сложность системы от него скрыта. Несмотря на то, что все апдейты кворума и лидера упорядочены в ZK, но сами реплики получают уведомления об этих событиях по сети от контроллера в произвольном порядке, поэтому в спеке явно моделируется взаимодействие реплик и сетевые сообщения от контроллера к репликам.

Percolator -- это протокол распределенных client-side транзакций над BigTable -- распределенных k/v хранилищем. Всё взаимодействие между клиентами происходит через BigTable, поэтому распределенность явно не моделируется, транзакции работают в модели разделяемой памяти.

### Моделирование участников
Не всегда надо явно моделировать сущности, которые производят действия в системе.

Вот какой пример приводит Лэмпорт: "Mathematical manipulation of specifications can yield new insight. A producer/consumer system can be written as the conjunction of two formulas, representing the producer and consumer processes. Simple mathematics allows us to rewrite the same specification as the conjunction of n formulas, each representing a single buffer element. ... Processes are not fundamental components of a system, but abstractions that we impose on it."

Для распределенных систем можно так же моделировать не самих участников, а сущности, с которыми они работают.

Например, в Paxos proposer исполняет бесконечный цикл, в котором он выбирает новый ballot и пытается предложить значение. При этом в спеке proposer-а и явного цикла нет, вместо этого моделируются отдельные proposal-ы:

    Next == \/ \E b \in Ballot : \/ Phase1a(b)
                                 \/ \E v \in Value : Phase2a(b, v)
            \/ \E a \in Acceptor : Phase1b(a) \/ Phase2b(a)

Другой пример - спека Spansphot Isolation, в которой не моделируются клиенты, а транзакции рождаются "из воздуха":

    Next == \/ \E txn \in TxnId :
                   (* Public actions *)
                \/ Begin(txn)
                \/ Commit(txn)
                \/ ChooseToAbort(txn)
                ...

### Моделирование мира
Займемся моделированием мира, в котором функционирует наша система: опишем, как сеть передает сообщения, как течет время, как отказывают узлы.

В теории распределенных вычислений выделяют две основные модели: *синхронную* и *асинхронную*. В асинхронной нет ограничений на:
* время доставки сообщений
* дрейф часов
* относительную скорость работы узлов.

В синхронной модели эти параметры ограничены.

Реальность где-то посередине. Например, реальная сеть большую часть времени ведет себя синхронно, но никакая сетевая инфраструктура не может гарантировать это на 100% времени. Бывают периоды нестабильности, когда сеть задерживает и теряет сообщения.

Мы будем использовать асинхронную модель. Мы сильно пессимизируем реальность, настоящая сеть не бывает асинхронной бесконечно долго. С другой стороны, если если мы убедимся в корректности алгоритма или системы в максимально сложной модели, то и в реальном мире получим только корректные поведения. Еще одна причина, по которой выбрана асинхронная модель -- выразить ее в TLA+ гораздо проще.

Поговорим подробнее про каждую из составляющих модели.

### Сеть и отправка сообщений
Сеть, через которую взаимодействуют узлы распределенной системы, – это провода и промежуточные устройства: коммутаторы и роутеры. Сообщение, отправленное в сеть, существует либо в виде сигнала внутри провода, либо в виде набора байт в буфере промежуточного устройства.

В TLA+ мы можем абстрагироваться от этих физических подробностей и представить сеть просто как множество in-flight сообщений:

    VARIABLE msgs

Для отправки сообщения добавляем его в это множество:

    Send(m) == msgs' = msgs \cup {m}

Пример:

Proposer в Single-Decree Paxos отправляет аксепторам сообщение Prepare(b):

    Phase1a(b) == /\ Send([type |-> "1a", bal |-> b])
                  /\ UNCHANGED <<maxBal, maxVBal, maxVal>>

Acceptor получает его и отправляет обратно Promise:

    Phase1b(a) == /\ \E m \in msgs :
                      /\ m.type = "1a"
                      /\ m.bal > maxBal[a]
                      /\ maxBal' = [maxBal EXCEPT ![a] = m.bal]
                      /\ Send([type |-> "1b", acc |-> a, bal |-> m.bal,
                                mbal |-> maxVBal[a], mval |-> maxVal[a]])
                  /\ UNCHANGED <<maxVBal, maxVal>>

В Kafka контроллер меняет назначает нового лидера и новый кворум, после чего отправляет уведомления репликам партиции:

    ControllerUpdateIsr(newLeader, newIsr) == \E newLeaderEpoch \in LeaderEpochSeq!IdSet :
        /\ LeaderEpochSeq!NextId(newLeaderEpoch)
        /\  LET newControllerState == [
                leader |-> newLeader,
                leaderEpoch |-> newLeaderEpoch,
                isr |-> newIsr]
            IN  /\ quorumState' = newControllerState
                /\ leaderAndIsrRequests' = leaderAndIsrRequests \union {newControllerState}

### Сбои сети
Асинхронная сеть может задерживать сообщения на произвольное время, терять их или дублировать или даже распадаться на несвязные сегменты (*партишены*).

Явно моделировать задержки или потери сообщений не нужно: model checker исследует все возможные траектории, в том числе те, в которых сообщения долго не доставляются или вообще не выбираются из множества msgs. Аналогично чекер автоматически проверит сценарии в которых сообщения не доставляются между группами узлов.

Для моделирования дублирования сетью сообщений достаточно не удалять их из множества msgs.

### Широковещательная рассылка
Большинство распределенных алгоритмов использует не просто точечную отправку сообщений, а *широковещательную рассылку* (*broadcast*).
* RequestVote и AppendEntries в Raft
* Prepare и Propose в Paxos

Чтобы моделировать отправку сообщений всем участникам системы можно, не указывать явных адресатов.

Например, в спеке Paxos в начале первой фазы в сеть помещается единственное сообщение, адресованное всем acceptor-ам сразу, в нем не указывается адресат:

    Send([type |-> "1a", bal |-> b])

В сообщениях, которые приходят от acceptor-а к proposer-у необходимо указывать адресата и отправителя. Отправитель указывается явно, чтобы proposer понимал, когда он соберет кворум ответов от acceptor-ов.

    Send([type |-> "1b", acc |-> a, bal |-> m.bal,
          mbal |-> maxVBal[a], mval |-> maxVal[a]])

Такие техники упрощают алгоритм по сравнению с реальным миром, но в то же время уменьшают гранулярность событий в модели, так что существует верояность потерять сценарии с гонками.

### Сбои узлов
Существуют 3 основные модели сбоев:
* *Отказы* -- узел взрывается и больше не принимает и не отправляет сообщения.
* *Рестарты* -- узел не отвечает некоторое время, а потом опять начинает участвовать в алгоритме.
* *Византийские сбои* -- сбойный узел ведет себя абсолютно произвольным образом - он попал под контроль злоумышленника или же нарушает протокол из-за ошибки в реализации алгоритма.

С моделированием отказов все просто: мы выбрали асинхронную модель, а в ней согласно FLP теореме невозможно отличить умерший узел от асинхронности сети

Алгоритм, устойчивый к рестартам узлов, должен хранить часть своего состояния в надежном персистентном хранилище.

Например, в RAFT реплика в фазе выбора лидера обязана надежно сохранять отданный голос перед ответом кандидату на его запрос RequestVote, в противном случае после рестарта она может проголосовать в том же терме во второй раз.

В Basic Paxos acceptor после Promise(b) на первой фазе должен игнорировать / отклонять сообщения с меньшими номерами. Для этого acceptor запоминает максимальный ballot number, который он получал от proposer-ов. Запоминать этот номер нужно обязательно в персистентом хранилище перед отправкой Promise / Accept, чтобы после рестарта не забыть о данном обещании.

В TLA+ можно промоделировать рестарт узла отдельным действием, который сбрасывает его волатильное состояние.

В спеке Raft рестарт реплики моделируется с помощью действия Restart:

    Restart(i) ==
    /\ state'          = [state EXCEPT ![i] = Follower]
    /\ votesResponded' = [votesResponded EXCEPT ![i] = {}]
    /\ votesGranted'   = [votesGranted EXCEPT ![i] = {}]
    /\ voterLog'       = [voterLog EXCEPT ![i] = [j \in {} |-> <<>>]]
    /\ nextIndex'      = [nextIndex EXCEPT ![i] = [j \in Server |-> 1]]
    /\ matchIndex'     = [matchIndex EXCEPT ![i] = [j \in Server |-> 0]]
    /\ commitIndex'    = [commitIndex EXCEPT ![i] = 0]
    /\ UNCHANGED <<messages, currentTerm, votedFor, log, elections>>


В этом примере персистентное состояние (currentTerm, votedFor, log) помещается в UNCHANCHED, волатильное (state, votesResponded, votesGranted) сбрасывается до начальных значений.

Моделирование византийских отказов требует отдельного изучения и в этой работе затронута не будет.

### Время
Узлы в распределенных системах используют время для заведения таймаутов, а таймауты - для обнаружения сбоев.

Например:

В Raft follower ждет определенное время (election timeout) сообщений от текущего лидера, и если не получает их, то считает текущего лидера "мертвым", переходит в новый терм и инициирует выборы нового лидера.

В спеке Raft для этого таймаута заведен отдельный экшн:

    Timeout(i) == /\ state[i] \in {Follower, Candidate}
                  /\ state' = [state EXCEPT ![i] = Candidate]
                  /\ currentTerm' = [currentTerm EXCEPT ![i] = currentTerm[i] + 1]
                  \* Most implementations would probably just set the local vote
                  \* atomically, but messaging localhost for it is weaker.
                  /\ votedFor' = [votedFor EXCEPT ![i] = Nil]
                  /\ votesResponded' = [votesResponded EXCEPT ![i] = {}]
                  /\ votesGranted'   = [votesGranted EXCEPT ![i] = {}]
                  /\ voterLog'       = [voterLog EXCEPT ![i] = [j \in {} |-> <<>>]]
                  /\ UNCHANGED <<messages, leaderVars, logVars>>

Никаких часов при этом явно не моделируется.

### Недетерминизм
Недетерминизм является одним из главных источников сложности проектирования распределенных систем. Недетерминизм возникает естественным образом из-за реактивной природы распределенных систем: они реагируют на внешние воздействия (запросы клиентов) и подвержены влиянию неконтролируемой среды (сеть, течение времени), железа / runtime-а  (паузы gc) и сбоев.

В TLA+ есть два основных способа выразить недетерминизм:
* С помощью квантора \E можно выбрать, какой из узлов системы следующим сделает "ход": получит сообщение, перезагрузится и т.п.
* С помощью дизъюнкции в действиях: например, клиент может отправить на узел БД либо запрос SELECT, либо INSERT:

    ClientAction == Insert \/ Select

Примеры:

В спеке Raft-а выбирается какое сообщение будет доставлено или какой из узлов выполнит действие

    Next == /\ \/ \E i \in Server : Restart(i)
               \/ \E i \in Server : Timeout(i)
               \/ \E m \in DOMAIN messages : Receive(m)
               \/ \E m \in DOMAIN messages : DuplicateMessage(m)


В спеке Paxos-а выбирается какое сообщение будет обработано или какой из acceptor-ов ответит на запрос

    Phase2b(a) == \E m \in msgs : /\ m.type = "2a"
                                  /\ m.bal \geq maxBal[a]
                                  /\ maxBal' = [maxBal EXCEPT ![a] = m.bal]
                                  /\ maxVBal' = [maxVBal EXCEPT ![a] = m.bal]
                                  /\ maxVal' = [maxVal EXCEPT ![a] = m.val]
                                  /\ Send([type |-> "2b", acc |-> a,
                                           bal |-> m.bal, val |-> m.val])

    Next == \/ \E b \in Ballot : \/ Phase1a(b)
                                \/ \E v \in Value : Phase2a(b, v)
           \/ \E a \in Acceptor : Phase1b(a) \/ Phase2b(a)

### Cвойства системы
Требования к системе / алгоритму задаются в виде свойств. Свойство – набор траекторий в графе состояний системы и формулируются в виде формул LTL.

Нас будут интересовать только 2 типа свойств:
* *Safety-свойства* - это свойства вида []P, где P - инвариант для отдельного состояния системы. Такие свойства говорят, что с системой не происходит "ничего плохо".
* *Liveness-свойства* - это свойства вида <>P, они говорят, что с системой в конце концов происходит что-то хорошее.

При верификации распределенных алгоритмов достаточно проверять только свойства двух приведенных типов: любое свойство можно выразить как пересечение свойств safety и liveness (https://pron.github.io/posts/tlaplus_part3#safety-and-liveness)

Приведем примеры safety свойств:

Пример инварианта в Raft: *Election Safety* в Raft -- в каждом терме может быть выбран не более чем один лидер.
В TLA+ это свойство выражено отрицанием инварианта MoreThanOneLeader

    BothLeader( i, j ) ==
         /\ i /= j
         /\ currentTerm[i] = currentTerm[j]
         /\ state[i] = Leader
         /\ state[j] = Leader

    MoreThanOneLeader ==
        \E i, j \in Server :  BothLeader( i, j )

    OneLeader == [](~MoreThanOneLeader)

В Paxos safety-свойство означает, что если предложение (b, v) выбрано (т.е. его приняло большинство аксепторов), то в любом предложении с большим номером может быть только то же самое значение:

    Agreed(v,b) == \E Q \in Quorum2: \A a \in Q: Sent2b(a,v,b)
    NoFutureProposal(v,b) == \A v2 \in Value: \A b2 \in Ballot: (b2 > b /\ Sent2a(v2,b2)) => v=v2

    SafeValue == \A v \in Value: \A b \in Ballot: Agreed(v,b) => NoFutureProposal(v,b)

    OnlyOneValue == []SafeValue

При проверки этого инварианта мы пользуемся тем, что сообщения из множества msgs не удаляются, а значит в каждом из состояний по нему можно восстановить все выбранные предложения.


В Kafka аппенды, которые подтверждены лидером, должны находиться в логах всех реплик кворума. В спеке TLA+ это свойство StrongIsr:

    StrongIsr == \A r1 \in Replicas :
        \/ ~ ReplicaPresumesLeadership(r1)
        \/ LET  hw == replicaState[r1].hw
           IN   \/ hw = 0
                \/ \A r2 \in quorumState.isr, offset \in 0 .. (hw - 1) : \E record \in LogRecords :
                    /\ ReplicaLog!HasEntry(r1, record, offset)
                    /\ ReplicaLog!HasEntry(r2, record, offset)

    Safety == []StrongIsr

Свойство WeakIsr отличается тем, в нем проверяется, что запись присутствует у всех реплик, которые текущая считает своим ISR.

Liveness-свойства мы рассматривать не будем:

Репликация в реальных распределенных системах как правило реализуется с помощью решения задачи распределенного консенсуса, а в асинхронной модели обеспечить завершаемость консенсуса невозможно в силу FLP-теоремы.

Если отказаться от консенсуса, то можно добиться завершаемости алгоритмов, но вместе с тем придется ослабить модель согласованности до eventual consistency, но ее мы не рассматриваем из-за слабых гарантий для пользователя.

### Проверка моделей согласованности
В предыдущем параграфе были приведены примеры safety-свойств. Если мы смотрим на систему не изнутри, а снаружи, то все алгоритмы от нас скрыты, нам доступно лишь наблюдаемое поведение, которое формулизуется в виде моделей согласованности.

Существует целый диапазон [https://jepsen.io/consistency] моделей согласованности для распределенных хранилищ. Но наиболее распространены две модели: *eventual consistency* и *linearizability*. Нас интересует в первую очередь самая сильная модель согласованности - линеаризуемость, поскольку она наиболее понятна пользователю. Для проверки линеаризуемости уже существует готовая спека https://github.com/lorin/tla-linearizability.
